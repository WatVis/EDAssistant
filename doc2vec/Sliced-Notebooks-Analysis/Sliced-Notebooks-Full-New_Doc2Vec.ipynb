{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "criminal-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json \n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ready-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/all-notebooks-sliced.csv\")\n",
    "df_usage = pd.read_csv(\"../data/all-notebooks-sliced-usage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impressive-montana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249315"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "capital-literature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'source', 'filename', 'competition', 'name', 'cell'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "judicial-spirituality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>filename</th>\n",
       "      <th>competition</th>\n",
       "      <th>name</th>\n",
       "      <th>cell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16246</th>\n",
       "      <td>142030</td>\n",
       "      <td>import pandas as pd\\nmacro_cols = [\"balance_tr...</td>\n",
       "      <td>1124467_1</td>\n",
       "      <td>sberbank-russian-housing-market</td>\n",
       "      <td>1124467</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154606</th>\n",
       "      <td>168465</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>29212989_1</td>\n",
       "      <td>Kannada-MNIST</td>\n",
       "      <td>29212989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103309</th>\n",
       "      <td>149817</td>\n",
       "      <td>import pandas as pd\\ntrain = pd.read_csv('../i...</td>\n",
       "      <td>14524975_7</td>\n",
       "      <td>jigsaw-unintended-bias-in-toxicity-classification</td>\n",
       "      <td>14524975</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238461</th>\n",
       "      <td>176495</td>\n",
       "      <td>import pandas as pd\\ndf_train = pd.read_csv('/...</td>\n",
       "      <td>45787949_3</td>\n",
       "      <td>rsna-str-pulmonary-embolism-detection</td>\n",
       "      <td>45787949</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14296</th>\n",
       "      <td>77309</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd\\nfrom ...</td>\n",
       "      <td>1080655_1</td>\n",
       "      <td>two-sigma-connect-rental-listing-inquiries</td>\n",
       "      <td>1080655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226001</th>\n",
       "      <td>39582</td>\n",
       "      <td>import pandas as pd\\ntrain = pd.read_csv('../i...</td>\n",
       "      <td>42422986_2</td>\n",
       "      <td>lish-moa</td>\n",
       "      <td>42422986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246008</th>\n",
       "      <td>39797</td>\n",
       "      <td>import numpy as np\\nimport pandas as pd \\nfrom...</td>\n",
       "      <td>48367818_13</td>\n",
       "      <td>lish-moa</td>\n",
       "      <td>48367818</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>59948</td>\n",
       "      <td>import pandas as pd\\nimport numpy as np\\nfrom ...</td>\n",
       "      <td>384264_4</td>\n",
       "      <td>talkingdata-mobile-user-demographics</td>\n",
       "      <td>384264</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39194</th>\n",
       "      <td>183440</td>\n",
       "      <td>import pandas as pd # data processing, CSV fil...</td>\n",
       "      <td>2914127_4</td>\n",
       "      <td>talkingdata-adtracking-fraud-detection</td>\n",
       "      <td>2914127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188984</th>\n",
       "      <td>15774</td>\n",
       "      <td>import math, re, os\\nimport tensorflow as tf\\n...</td>\n",
       "      <td>33709569_10</td>\n",
       "      <td>flower-classification-with-tpus</td>\n",
       "      <td>33709569</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                             source  \\\n",
       "16246       142030  import pandas as pd\\nmacro_cols = [\"balance_tr...   \n",
       "154606      168465  import pandas as pd # data processing, CSV fil...   \n",
       "103309      149817  import pandas as pd\\ntrain = pd.read_csv('../i...   \n",
       "238461      176495  import pandas as pd\\ndf_train = pd.read_csv('/...   \n",
       "14296        77309  import numpy as np\\nimport pandas as pd\\nfrom ...   \n",
       "226001       39582  import pandas as pd\\ntrain = pd.read_csv('../i...   \n",
       "246008       39797  import numpy as np\\nimport pandas as pd \\nfrom...   \n",
       "5919         59948  import pandas as pd\\nimport numpy as np\\nfrom ...   \n",
       "39194       183440  import pandas as pd # data processing, CSV fil...   \n",
       "188984       15774  import math, re, os\\nimport tensorflow as tf\\n...   \n",
       "\n",
       "           filename                                        competition  \\\n",
       "16246     1124467_1                    sberbank-russian-housing-market   \n",
       "154606   29212989_1                                      Kannada-MNIST   \n",
       "103309   14524975_7  jigsaw-unintended-bias-in-toxicity-classification   \n",
       "238461   45787949_3              rsna-str-pulmonary-embolism-detection   \n",
       "14296     1080655_1         two-sigma-connect-rental-listing-inquiries   \n",
       "226001   42422986_2                                           lish-moa   \n",
       "246008  48367818_13                                           lish-moa   \n",
       "5919       384264_4               talkingdata-mobile-user-demographics   \n",
       "39194     2914127_4             talkingdata-adtracking-fraud-detection   \n",
       "188984  33709569_10                    flower-classification-with-tpus   \n",
       "\n",
       "            name  cell  \n",
       "16246    1124467     1  \n",
       "154606  29212989     1  \n",
       "103309  14524975     7  \n",
       "238461  45787949     3  \n",
       "14296    1080655     1  \n",
       "226001  42422986     2  \n",
       "246008  48367818    13  \n",
       "5919      384264     4  \n",
       "39194    2914127     4  \n",
       "188984  33709569    10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dense-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-recording",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Generate input data from raw source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "danish-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "class MyDataframeCorpus(object):\n",
    "    def __init__(self, source_df, text_col, tag_col):\n",
    "        self.source_df = source_df\n",
    "        self.text_col = text_col\n",
    "        self.tag_col = tag_col\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, row in self.source_df.iterrows():\n",
    "            yield TaggedDocument(words=simple_preprocess(row[self.text_col]), \n",
    "                                 tags=[row[self.tag_col]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-conference",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "saving-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = MyDataframeCorpus(subdf, 'source', 'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "working-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "reserved-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "amber-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-vacation",
   "metadata": {},
   "source": [
    "## Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "labeled-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "testinput = \"pca.fit()\"\n",
    "vector = model.infer_vector(testinput.split(\" \"))\n",
    "# print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "federal-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('401386_9', 0.18951842188835144), ('349234_28', 0.17214050889015198), ('401386_6', 0.17143431305885315), ('349234_29', 0.15756113827228546), ('349234_47', 0.15369239449501038), ('349234_46', 0.1498398333787918), ('401386_7', 0.1402955800294876), ('157415_0', 0.13813436031341553), ('135231_0', 0.13525982201099396), ('458351_11', 0.134013831615448)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6526    import pandas\\nfrom sklearn.cross_validation import train_test_split\\nimport numpy\\nimport xgboost as xgb\\ntrain=pandas.read_csv(\"../input/train.csv\")\\ntraining,testing = train_test_split(train,test_size=0.2,random_state=42)\\ntraining=training.reset_index(drop=True)\\ntesting = testing.reset_index(drop=True)\\ntraining['logloss']=numpy.log(training['loss'])\\nfeatures = training.columns\\ncat_feature=list(features[0:116])\\nfor each in cat_feature:\\n    training[each]=pandas.factorize(training[each], sort=True)[0]\\n    testing[each]=pandas.factorize(testing[each],sort=True)[0]\\nfrom sklearn.metrics import mean_absolute_error as mae\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.tree import DecisionTreeRegressor\\nPredictors= training.ix[:,0:130]\\nPredictors_test= testing.ix[:,0:130]\\nRegressors = [LinearRegression(),Lasso(),DecisionTreeRegressor()\\n              #,RandomForestRegressor(n_estimator=200),\\n              #GradientBoostingRegressor(learning_rate=0.3,criterion='mae')\\n             ]\\nfor reg in Regressors:\\n    Model=reg.fit(Predictors,training['logloss'])\\n    Prediction= numpy.exp(Model.predict(Predictors_test))\\ntraining_array = numpy.array(Predictors)\\ntesting_array = numpy.array(Predictors_test)\\ndtrain = xgb.DMatrix(training_array, label=training['logloss'])\\ndtest = xgb.DMatrix(testing_array)\\nxgb_params = {\\n    'seed':0,\\n    'colsample_bytree': 0.7,\\n    'subsample': 0.7,\\n    'learning_rate': 0.075,\\n    'objective': 'reg:linear',\\n    'max_depth': 6,\\n    'min_child_weight': 1,\\n    'eval_metric': 'mae',\\n}\\nxgb_model=xgb.train(xgb_params, dtrain,750,verbose_eval=50)\\nxgb_pred=numpy.exp(xgb_model.predict(dtest))\\nprint('Accuracy of XGboost model is'+' '+str(mae(testing['loss'],xgb_pred)))\\n\n",
       "Name: source, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5316    import pandas as pd\\nimport numpy as np\\nfrom sklearn import mixture\\nfrom sklearn import ensemble\\nfrom sklearn import cross_validation\\nfrom sklearn.metrics import accuracy_score as accuracy\\nfrom sklearn.metrics import log_loss\\nimport time\\nallData = pd.read_csv('../input/data.csv')\\ndata = allData[allData['shot_made_flag'].notnull()].reset_index()\\ndata['game_date_DT'] = pd.to_datetime(data['game_date'])\\ndata['dayOfWeek'] = data['game_date_DT'].dt.dayofweek\\ndata['dayOfYear'] = data['game_date_DT'].dt.dayofyear\\ndata['secondsFromPeriodEnd'] = 60*data['minutes_remaining']+data['seconds_remaining']\\ndata['secondsFromPeriodStart'] = 60*(11-data['minutes_remaining'])+(60-data['seconds_remaining'])\\ndata['secondsFromGameStart'] = (data['period'] <= 4).astype(int)*(data['period']-1)*12*60 + (data['period'] > 4).astype(int)*((data['period']-4)*5*60 + 3*12*60) + data['secondsFromPeriodStart']\\nnumGaussians = 13\\ngaussianMixtureModel = mixture.GMM(n_components=numGaussians, covariance_type='full', \\n                                   params='wmc', init_params='wmc',\\n                                   random_state=1, n_init=3,  verbose=0)\\ngaussianMixtureModel.fit(data.ix[:,['loc_x','loc_y']])\\ndata['shotLocationCluster'] = gaussianMixtureModel.predict(data.ix[:,['loc_x','loc_y']])\\ndef FactorizeCategoricalVariable(inputDB,categoricalVarName):\\n    opponentCategories = inputDB[categoricalVarName].value_counts().index.tolist()\\n    \\n    outputDB = pd.DataFrame()\\n    for category in opponentCategories:\\n        featureName = categoricalVarName + ': ' + str(category)\\n        outputDB[featureName] = (inputDB[categoricalVarName] == category).astype(int)\\n    return outputDB\\nfeaturesDB = pd.DataFrame()\\nfeaturesDB['homeGame'] = data['matchup'].apply(lambda x: 1 if (x.find('@') < 0) else 0)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'opponent')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'action_type')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_type')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'combined_shot_type')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_zone_basic')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_zone_area')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shot_zone_range')],axis=1)\\nfeaturesDB = pd.concat([featuresDB,FactorizeCategoricalVariable(data,'shotLocationCluster')],axis=1)\\nfeaturesDB['timeOfYear_cycY'] = np.cos(2*np.pi*(data['dayOfYear']/365))\\nlabelsDB = data['shot_made_flag']\\nrandomSeed = 1\\nnumFolds = 4\\nmainLearner = ensemble.ExtraTreesClassifier(n_estimators=500, max_depth=5, \\n                                            min_samples_leaf=120, max_features=120, \\n                                            criterion='entropy', bootstrap=False, \\n                                            n_jobs=-1, random_state=randomSeed)\\ncrossValidationIterator = cross_validation.StratifiedKFold(labelsDB, n_folds=numFolds, \\n                                                           shuffle=True, random_state=randomSeed)\\nstartTime = time.time()\\ntrainAccuracy = []; validAccuracy = [];\\ntrainAccuracy = []; validAccuracy = [];\\ntrainLogLosses = []; validLogLosses = []\\ntrainLogLosses = []; validLogLosses = []\\nfor trainInds, validInds in crossValidationIterator:\\n    X_train_CV = featuresDB.ix[trainInds,:]\\n    y_train_CV = labelsDB.iloc[trainInds]\\n    X_valid_CV = featuresDB.ix[validInds,:]\\n    y_valid_CV = labelsDB.iloc[validInds]\\n    mainLearner.fit(X_train_CV, y_train_CV)\\n    y_train_hat_mainLearner = mainLearner.predict_proba(X_train_CV)[:,1]\\n    y_valid_hat_mainLearner = mainLearner.predict_proba(X_valid_CV)[:,1]\\n    trainAccuracy.append(accuracy(y_train_CV, y_train_hat_mainLearner > 0.5))\\n    validAccuracy.append(accuracy(y_valid_CV, y_valid_hat_mainLearner > 0.5))\\n    trainLogLosses.append(log_loss(y_train_CV, y_train_hat_mainLearner))\\n    validLogLosses.append(log_loss(y_valid_CV, y_valid_hat_mainLearner))\\nprint(\"total (train,valid) Accuracy = (%.5f,%.5f). took %.2f minutes\" % (np.mean(trainAccuracy),np.mean(validAccuracy), (time.time()-startTime)/60))\\nprint(\"total (train,valid) Log Loss = (%.5f,%.5f). took %.2f minutes\" % (np.mean(trainLogLosses),np.mean(validLogLosses), (time.time()-startTime)/60))\\n\n",
       "Name: source, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6523    import pandas\\nfrom sklearn.cross_validation import train_test_split\\nimport numpy\\ntrain=pandas.read_csv(\"../input/train.csv\")\\ntraining,testing = train_test_split(train,test_size=0.2,random_state=42)\\ntraining=training.reset_index(drop=True)\\ntesting = testing.reset_index(drop=True)\\ntraining['logloss']=numpy.log(training['loss'])\\nfeatures = training.columns\\ncat_feature=list(features[0:116])\\nfor each in cat_feature:\\n    training[each]=pandas.factorize(training[each], sort=True)[0]\\n    testing[each]=pandas.factorize(testing[each],sort=True)[0]\\nfrom sklearn.metrics import mean_absolute_error as mae\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.tree import DecisionTreeRegressor\\nPredictors= training.ix[:,0:130]\\nPredictors_test= testing.ix[:,0:130]\\nRegressors = [LinearRegression(),Lasso(),DecisionTreeRegressor()\\n              #,RandomForestRegressor(n_estimator=200),\\n              #GradientBoostingRegressor(learning_rate=0.3,criterion='mae')\\n             ]\\nMAE=[]\\nModel_Name=[]\\nfor reg in Regressors:\\n    Model=reg.fit(Predictors,training['logloss'])\\n    Prediction= numpy.exp(Model.predict(Predictors_test))\\n    eva = mae(testing['loss'],Prediction)\\n    MAE.append(eva)\\n    Name=reg.__class__.__name__\\n    Model_Name.append(Name)\\n    print('Accuracy of'+ ' '+Name+' '+'is'+' '+str(eva))\\n\n",
       "Name: source, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sims = model.docvecs.most_similar([vector])\n",
    "print(sims)\n",
    "\n",
    "for result in sims[0:3]:\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        display(df[df.filename == result[0]]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-cheese",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "velvet-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/sliced-cells-notebook-doc2vec-model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-ladder",
   "metadata": {},
   "source": [
    "# Do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "substantial-register",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199505    import pandas as pd\\nDATA_PATH = '/kaggle/input/trends-assessment-prediction/'\\ny_train = pd.read_csv(DATA_PATH + 'train_scores.csv')\\nprint('Number of training samples: {}'.format(len(y_train)))\\n\n",
      "Name: source, dtype: object\n",
      "201946    import pandas as pd\\nDATA_PATH = '/kaggle/input/trends-assessment-prediction/'\\ny_train = pd.read_csv(DATA_PATH + 'train_scores.csv')\\nprint('Number of training samples: {}'.format(len(y_train)))\\n\n",
      "Name: source, dtype: object\n",
      "232355    from imblearn.over_sampling import RandomOverSampler\\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\ntrain = pd.read_csv('/kaggle/input/challenges-in-representation-learning-facial-expression-recognition-challenge/train.csv')\\noversample = RandomOverSampler(sampling_strategy='auto')\\nX_over, y_over = oversample.fit_resample((train.pixels).values.reshape(-1, 1), train.emotion)\\ny_over = pd.Series(y_over)\\ny_over= y_over.values.reshape(len(y_over),1)\\nX_over = pd.Series(X_over.flatten())\\nY_train = y_over\\nprint (\"Y_train shape: \" + str(Y_train.shape))\\n\n",
      "Name: source, dtype: object\n",
      "199506    import pandas as pd\\nDATA_PATH = '/kaggle/input/trends-assessment-prediction/'\\ny_train = pd.read_csv(DATA_PATH + 'train_scores.csv')\\ny_train.head()\\n\n",
      "Name: source, dtype: object\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "import pandas\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "tf.fit()\n",
    "\"\"\"\n",
    "vector = model.infer_vector(query.split(\" \"))\n",
    "\n",
    "\n",
    "sims = model.docvecs.most_similar([vector])\n",
    "# print(sims)\n",
    "\n",
    "# Lookup top 5 similar \n",
    "# Lookup the notebook filename in the dataframe\n",
    "for result in sims[0:4]:\n",
    "    print(df[df.filename == result[0]]['source'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

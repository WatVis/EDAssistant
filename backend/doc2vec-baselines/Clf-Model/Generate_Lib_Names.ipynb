{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "overall-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "colored-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"../doc2vec/data/all-notebooks-tokenized.json\", orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "funny-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_dict = pickle.load(open(\"lib_dict.pkl\",'rb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "demonstrated-transcript",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15478"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lib_dict['pandas.read_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minus-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdf = data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "important-blast",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'subprocess', 'import', 'check_output', '[NEWLINE]', '[INDENT]', 'import', 'numpy', 'as', 'np', '# linear algebra', '[NEWLINE]', 'import', 'pandas', 'as', 'pd', '# data processing, CSV file I/O (e.g. pd.read_csv)', '[NEWLINE]', 'print', '(', 'check_output', '(', '[', '\"ls\"', ',', '\"../input\"', ']).decode', '(', '\"utf8\"', ')', ')', '[NEWLINE]', 'import', 're', '[NEWLINE]', 'from', 'datetime', 'import', 'datetime', 'as', 'dt', '[NEWLINE]', 'from', 'datetime', 'import', 'timedelta# Any results you write to the current directory are saved as output.[NEWLINE]', '', '']\n",
      "['holiday', '=pd.read_csv', '(', \"'../input/holidays_events.csv'\", ')', '[NEWLINE]', '[INDENT]', 'stores', '=pd.read_csv', '(', \"'../input/stores.csv'\", ')', '[NEWLINE]', '', '']\n",
      "['# converting date into datetime format', '[NEWLINE]', '[INDENT]', 'holiday', '[', \"'date'\", ']', '=pd.to_datetime', '(', 'holiday', '[', \"'date'\", ']', ')', '[NEWLINE]', 'print', '(', 'holiday', '[', \"'description'].unique\", '(', ')', ')', '[NEWLINE]', '', '']\n",
      "['# all bridge-type has puente (puente actually mean bridge), will remove it ', '[NEWLINE]', '# all work day type has recupero (recupero means recovery), will remove it', '[NEWLINE]', '# almost all Transfer have traslado (it actually means transfer), will remove it', '[NEWLINE]', '# as all of these information are already contained in type', '[NEWLINE]', '[INDENT]', 'hol_groups', '=holiday.groupby', '(', \"'type'\", ')', '[NEWLINE]', 'for', 'c', ',', 'group', 'in', 'hol_groups', ':', '[NEWLINE]', '[INDENT]', 'print', '(', 'c', ',', 'group', '[', \"'description'].unique\", '(', ')', ')', '[NEWLINE]', '', '', '']\n",
      "['# for Regional and Local holidays, ', '[NEWLINE]', '# name locale_name is part of description as well (will remove that)', '[NEWLINE]', '# as this info is already contained in locale_name', '[NEWLINE]', '[INDENT]', 'hol_groups2', '=', 'holiday.groupby', '(', '[', \"'locale'\", ',', \"'locale_name'\", ']', ')', '[NEWLINE]', 'for', 'c', ',', 'group', 'in', 'hol_groups2', ':', '[NEWLINE]', '[INDENT]', 'print', '(', 'c', ',', 'group', '[', \"'description'\", '].unique', '(', ')', ')', '[NEWLINE]', '', '', '']\n",
      "['# number just mean consecutive events which is already given by there dates, ', '[NEWLINE]', '# hence will remove them from description', '[NEWLINE]', '[INDENT]', 'navidadl', '=', '[', \"'Navidad-4'\", ',', \"'Navidad-3'\", ',', \"'Navidad-2'\", ',', \"'Navidad-1'\", ',', \"'Navidad'\", ',', \"'Navidad+1'\", ']', '[NEWLINE]', 'print', '(', 'holiday', '[', 'holiday', '[', \"'description'\", '].isin', '(', 'navidadl', ')', ']', '[', '[', \"'date'\", ',', \"'description'\", ']', ']', ')', '[NEWLINE]', 'terr_hol', '=', '[', \"'Terremoto Manabi'\", ',', \"'Terremoto Manabi+1'\", ',', \"'Terremoto Manabi+2'\", ',', '[NEWLINE]', \"'Terremoto Manabi+3'\", ',', \"'Terremoto Manabi+4'\", ',', \"'Terremoto Manabi+5'\", ',', '[NEWLINE]', \"'Terremoto Manabi+6'\", ',', \"'Terremoto Manabi+7'\", ',', \"'Terremoto Manabi+8'\", ',', '[NEWLINE]', \"'Terremoto Manabi+9'\", ',', \"'Terremoto Manabi+10'\", ',', \"'Terremoto Manabi+11'\", ',', '[NEWLINE]', \"'Terremoto Manabi+12'\", ',', \"'Terremoto Manabi+13'\", ',', \"'Terremoto Manabi+14'\", ',', '[NEWLINE]', \"'Terremoto Manabi+15'\", ',', \"'Terremoto Manabi+16'\", ',', \"'Terremoto Manabi+17'\", ',', '[NEWLINE]', \"'Terremoto Manabi+18'\", ',', \"'Terremoto Manabi+19'\", ',', \"'Terremoto Manabi+20'\", ',', '[NEWLINE]', \"'Terremoto Manabi+21'\", ',', \"'Terremoto Manabi+22'\", ',', \"'Terremoto Manabi+23'\", ',', '[NEWLINE]', \"'Terremoto Manabi+24'\", ',', \"'Terremoto Manabi+25'\", ',', \"'Terremoto Manabi+26'\", ',', '[NEWLINE]', \"'Terremoto Manabi+27'\", ',', \"'Terremoto Manabi+28'\", ',', \"'Terremoto Manabi+29'\", ',', '[NEWLINE]', \"'Terremoto Manabi+30'\", ']', '[NEWLINE]', 'print', '(', 'holiday', '[', 'holiday', '[', \"'description'\", '].isin', '(', 'terr_hol', ')', ']', '[', '[', \"'date'\", ',', \"'description'\", ']', ']', ')', '[NEWLINE]', '', '']\n",
      "['# de and del are just stop words, will remove them too', '[NEWLINE]', '# seriously reduces dimensionality of description variable', '[NEWLINE]', '[INDENT]', 'holiday', '[', \"'description'\", ']', '=', 'holiday', '[', \"'description'\", '].str.lower', '(', ').replace', '(', 'to_replace', '=', \"'[^a-z ]'\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ')', '[NEWLINE]', 'holiday', '[', \"'description'\", ']', '=', 'holiday.apply', '(', 'lambda', 'x', ':', 'x', '[', \"'description'\", '].replace', '(', 'x', '[', \"'locale_name'\", '].lower', '(', ')', ',', \"''\", ')', ',', 'axis', '=', '1', ')', '[NEWLINE]', 'holiday', '[', \"'description'\", ']', '=', 'holiday', '[', \"'description'\", '].replace', '(', 'to_replace', '=', \"'traslado '\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ').replace', '(', 'to_replace', '=', \"'puente '\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ').replace', '(', 'to_replace', '=', \"'del '\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ').replace', '(', 'to_replace', '=', \"'de '\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ').replace', '(', 'to_replace', '=', \"'mundial futbol brasil'\", ',', 'value', '=', \"'mfb'\", ',', 'regex', '=', 'True', ').replace', '(', 'to_replace', '=', \"'recupero '\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ').replace', '(', 'to_replace', '=', \"'santo domingo'\", ',', 'value', '=', \"''\", ',', 'regex', '=', 'True', ')', '[NEWLINE]', 'holiday', '[', \"'description'\", ']', '=', 'holiday', '[', \"'description'\", '].apply', '(', 'lambda', 'x', ':', 'x.strip', '(', ')', ')', '[NEWLINE]', 'print', '(', 'holiday', '[', \"'description'\", '].unique', '(', ')', ')', '[NEWLINE]', '', '']\n",
      "[\"# now some holidays are city level and will impact only that city store (locale == 'Local')\", '[NEWLINE]', \"# and some are state level and will only impact store in that state (locale == 'Regional')\", '[NEWLINE]', '# and national onces will impact all of stores in Ecuador', '[NEWLINE]', '# and hence city level and state level will need us to merge store and date some how', '[NEWLINE]', '## Creating date only dataframe', '[NEWLINE]', '[INDENT]', 'start_dt', '=', 'dt', '(', 'year', '=', '2013', ',', 'month', '=', '1', ',', 'day', '=', '1', ')', '[NEWLINE]', 'dt_only', '=', 'pd.DataFrame', '(', '[', 'start_dt', '+', 'timedelta', '(', 'days', '=', 'i', ')', 'for', 'i', 'in', 'range', '(', '1704', ')', ']', ',', 'columns', '=', '[', \"'date'\", ']', ')', '[NEWLINE]', 'print', '(', 'dt_only.max', '(', ')', ')', '[NEWLINE]', 'print', '(', 'dt_only.min', '(', ')', ')', '[NEWLINE]', '', '']\n",
      "['# taking a cross product of store and date', '[NEWLINE]', '[INDENT]', 'dt_only', '[', \"'dummy'\", ']', '=', '1', '[NEWLINE]', 'stores', '[', \"'dummy'\", ']', '=', '1', '[NEWLINE]', 'dt_store', '=', 'pd.merge', '(', 'dt_only', ',', 'stores', ',', 'on', '=', '[', \"'dummy'\", ']', ').drop', '(', \"'dummy'\", ',', 'axis', '=', '1', ')', '[NEWLINE]', 'print', '(', 'dt_store.head', '(', '10', ')', ')', '[NEWLINE]', '', '']\n",
      "['# seperating out National, city and state level holidays', '[NEWLINE]', '[INDENT]', 'national_h', '=', 'holiday', '[', 'holiday', '[', \"'locale'\", ']', '==', \"'National'\", '].drop', '(', '[', \"'locale'\", ',', \"'locale_name'\", ']', ',', 'axis', '=', '1', ')', '[NEWLINE]', 'national_h.columns', '=', '[', \"'date'\", ',', \"'ntype'\", ',', \"'ndescription'\", ',', \"'ntransfered'\", ']', '[NEWLINE]', 'city_h', '=', 'holiday', '[', 'holiday', '[', \"'locale'\", ']', '==', \"'Local'\", '].drop', '(', \"'locale'\", ',', 'axis', '=', '1', ')', '[NEWLINE]', 'city_h.columns', '=', '[', \"'date'\", ',', \"'ctype'\", ',', \"'city'\", ',', \"'cdescription'\", ',', \"'ctransfered'\", ']', '[NEWLINE]', 'state_h', '=', 'holiday', '[', 'holiday', '[', \"'locale'\", ']', '==', \"'Regional'\", '].drop', '(', \"'locale'\", ',', 'axis', '=', '1', ')', '[NEWLINE]', 'state_h.columns', '=', '[', \"'date'\", ',', \"'stype'\", ',', \"'state'\", ',', \"'sdescription'\", ',', \"'stransfered'\", ']', '[NEWLINE]', 'print', '(', 'national_h.head', '(', ')', ')', '[NEWLINE]', 'print', '(', 'city_h.head', '(', ')', ')', '[NEWLINE]', 'print', '(', 'state_h.head', '(', ')', ')', '[NEWLINE]', '', '']\n",
      "['# merging with date store dataframe  ', '[NEWLINE]', '[INDENT]', 'dt_store_nat', '=', 'pd.merge', '(', 'dt_store', ',', 'national_h', ',', 'on', '=', \"'date'\", ',', 'how', '=', \"'left'\", ')', '[NEWLINE]', 'print', '(', 'dt_store_nat.shape', ')', '[NEWLINE]', 'dt_store_nat_ct', '=', 'pd.merge', '(', 'dt_store_nat', ',', 'city_h', ',', 'on', '=', '[', \"'date'\", ',', \"'city'\", ']', ',', 'how', '=', \"'left'\", ')', '[NEWLINE]', 'print', '(', 'dt_store_nat_ct.shape', ')', '[NEWLINE]', 'dt_store_nat_ct_st', '=', 'pd.merge', '(', 'dt_store_nat_ct', ',', 'state_h', ',', 'on', '=', '[', \"'date'\", ',', \"'state'\", ']', ',', 'how', '=', \"'left'\", ')', '[NEWLINE]', 'print', '(', 'dt_store_nat_ct_st.shape', ')', '[NEWLINE]', 'print', '(', 'dt_store_nat_ct_st.head', '(', ')', ')', '[NEWLINE]', '', '']\n",
      "['# You can use holiday features now after on hot encoding or whatever you would like to do', '[NEWLINE]', '# you can choose to keep only one of (national,city,state) features by using some hirachial ordering', '[NEWLINE]', \"# now you can use this dataframe to merge with main data set on ['date','store_nbr']\", '[NEWLINE]', '# Note: the increase in number of records on merging city_h', '[NEWLINE]', '# happening because of transfer and Additional on falling on same date', '[NEWLINE]', \"# My suggestion is to remove record with ctype = 'Transfer' and city = 'Guayaquil'\", '[NEWLINE]', '[INDENT]', 'citygroups', '=', 'city_h.groupby', '(', '[', \"'date'\", ',', \"'city'\", ']', ')', '[NEWLINE]', 'for', 'c', ',', 'group', 'in', 'citygroups', ':', '[NEWLINE]', '[INDENT]', 'if', 'group.shape', '[', '0', ']', '>', '1', ':', '[NEWLINE]', '[INDENT]', 'print', '(', 'group', ')', '[NEWLINE]', '', '', '', '']\n",
      "['']\n",
      "[]\n",
      "[]\n",
      "['import', 'numpy', '[NEWLINE]', '[INDENT]', 'import', 'matplotlib.pyplot', 'as', 'plt', '[NEWLINE]', 'import', 'pandas', 'as', 'pd', '[NEWLINE]', 'import', 'math', '[NEWLINE]', 'from', 'keras.models', 'import', 'Sequential', '[NEWLINE]', 'from', 'keras.layers', 'import', 'Dense', ',', 'Dropout', '[NEWLINE]', 'from', 'keras.layers', 'import', 'LSTM', '[NEWLINE]', 'from', 'sklearn.preprocessing', 'import', 'MinMaxScaler', '[NEWLINE]', 'from', 'sklearn.metrics', 'import', 'mean_squared_error', '[NEWLINE]', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['from', 'subprocess', 'import', 'check_output', '[NEWLINE]', '[INDENT]', 'print', '(', 'check_output', '(', '[', '\"ls\"', ',', '\"../input\"', ']', ').decode', '(', '\"utf8\"', ')', ')', '[NEWLINE]', '', '']\n",
      "['train', '=', 'pd.read_csv', '(', '\"../input/train.csv\"', ',', 'parse_dates', '=', '[', \"'date'\", ']', ')', '[NEWLINE]', '#test=pd.read_csv(\"../input/test.csv\",parse_dates=[\\'date\\'])', '[NEWLINE]', '[INDENT]', 'stores', '=', 'pd.read_csv', '(', '\"../input/stores.csv\"', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['t', '=', 'train.groupby', '(', '[', \"'store_nbr'\", ',', \"'date'\", ']', ',', 'as_index', '=', 'False', ').agg', '(', '{', '\"unit_sales\"', ':', '\"sum\"', '}', ')', '[NEWLINE]', '[INDENT]', 'train', '=', 'pd.merge', '(', 't', ',', 'stores', ',', 'how', '=', \"'left'\", ',', 'on', '=', '[', \"'store_nbr'\", ']', ')', '[NEWLINE]', 'mask', '=', 'train', '[', \"'state'\", ']', '==', \"'Pichincha'\", '[NEWLINE]', 'train', '=', 'train.loc', '[', 'mask', ']', '[NEWLINE]', 'train', '=', 'train.groupby', '(', '[', \"'date'\", ']', ',', 'as_index', '=', 'False', ').agg', '(', '{', '\"unit_sales\"', ':', '\"sum\"', '}', ')', '[NEWLINE]', '', '']\n",
      "['train', '.head', '(', '10', ')', '[NEWLINE]', '']\n",
      "['']\n",
      "['#train1.groupby(\\'month\\', as_index=False).agg({\"unit_sales\": \"sum\"})', '[NEWLINE]', '[NEWLINE]', '#MUPI_COM[\"valor\"].plot()', '[NEWLINE]', '[NEWLINE]', '']\n",
      "['']\n",
      "['']\n",
      "[]\n",
      "['train', '.tail', '(', '12', ')', '[NEWLINE]', '']\n",
      "['[NEWLINE]', '[INDENT]', 'plt.plot', '(', 'train', '[', \"'unit_sales'\", ']', ')', '[NEWLINE]', 'plt.show', '(', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "[]\n",
      "['numpy', '.random.seed', '(', '123', ')', '[NEWLINE]', '']\n",
      "['train_size', '=', 'int', '(', 'len', '(', 'train', ')', '*', '0.75', ')', '[NEWLINE]', '[INDENT]', 'test_size', '=', 'len', '(', 'train', ')', '-', 'train_size', '[NEWLINE]', '[NEWLINE]', 'print', '(', 'train_size', ',', 'test_size', ',', 'len', '(', 'train', ')', ')', '[NEWLINE]', '', '']\n",
      "['train1', '=', 'train', '[', '0', ':', 'train_size', ']', '[NEWLINE]', '[INDENT]', 'test', '=', 'train', '[', 'train_size', ':', 'len', '(', 'train', ')', ']', '[NEWLINE]', 'print', '(', 'len', '(', 'train1', ')', ',', 'len', '(', 'test', ')', ')', '[NEWLINE]', '', '']\n",
      "['train1', '=', 'train1.set_index', '(', '\"date\"', ')', '[NEWLINE]', '[INDENT]', 'test', '=', 'test.set_index', '(', '\"date\"', ')', '[NEWLINE]', 'train', '=', 'train.set_index', '(', '\"date\"', ')', '[NEWLINE]', 'train1', '=', 'train1.values', '[NEWLINE]', 'test', '=', 'test.values', '[NEWLINE]', 'train', '=', 'train.values', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['def', 'create_dataset', '(', 'dataset', ',', 'look_back', '=', '1', ')', ':', '[NEWLINE]', '[INDENT]', 'dataX', ',', 'dataY', '=', '[', ']', ',', '[', ']', '[NEWLINE]', 'for', 'i', 'in', 'range', '(', 'len', '(', 'dataset', ')', '-', 'look_back', '-', '1', ')', ':', '[NEWLINE]', '[INDENT]', 'a', '=', 'dataset', '[', 'i', ':', '(', 'i', '+', 'look_back', ')', ',', '0', ']', '[NEWLINE]', 'dataX.append', '(', 'a', ')', '[NEWLINE]', 'dataY.append', '(', 'dataset', '[', 'i', '+', 'look_back', ',', '0', ']', ')', '[NEWLINE]', '', 'return', 'numpy.array', '(', 'dataX', ')', ',', 'numpy.array', '(', 'dataY', ')', '[NEWLINE]', '', '']\n",
      "['[NEWLINE]', '[INDENT]', 'look_back', '=', '1', '[NEWLINE]', 'trainX', ',', 'trainY', '=', 'create_dataset', '(', 'train1', ',', 'look_back', ')', '[NEWLINE]', 'testX', ',', 'testY', '=', 'create_dataset', '(', 'test', ',', 'look_back', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['[NEWLINE]', '[INDENT]', 'model', '=', 'Sequential', '(', ')', '[NEWLINE]', 'model.add', '(', 'Dense', '(', '8', ',', 'input_dim', '=', 'look_back', ',', 'activation', '=', \"'relu'\", ')', ')', '[NEWLINE]', 'model.add', '(', 'Dense', '(', '1', ')', ')', '[NEWLINE]', 'model.compile', '(', 'loss', '=', \"'mean_squared_error'\", ',', 'optimizer', '=', \"'adam'\", ')', '[NEWLINE]', 'model.fit', '(', 'trainX', ',', 'trainY', ',', 'epochs', '=', '200', ',', 'batch_size', '=', '2', ',', 'verbose', '=', '2', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['trainScore', '=', 'model.evaluate', '(', 'trainX', ',', 'trainY', ',', 'verbose', '=', '0', ')', '[NEWLINE]', '[INDENT]', 'print', '(', \"'Train Score: %.2f MSE (%.2f RMSE)'\", '%', '(', 'trainScore', ',', 'math.sqrt', '(', 'trainScore', ')', ')', ')', '[NEWLINE]', 'testScore', '=', 'model.evaluate', '(', 'testX', ',', 'testY', ',', 'verbose', '=', '0', ')', '[NEWLINE]', 'print', '(', \"'Test Score: %.2f MSE (%.2f RMSE)'\", '%', '(', 'testScore', ',', 'math.sqrt', '(', 'testScore', ')', ')', ')', '[NEWLINE]', '', '']\n",
      "['trainPredict', '=', 'model.predict', '(', 'trainX', ')', '[NEWLINE]', '[INDENT]', 'testPredict', '=', 'model.predict', '(', 'testX', ')', '[NEWLINE]', '[NEWLINE]', 'trainPredictPlot', '=', 'numpy.empty_like', '(', 'train', ')', '[NEWLINE]', 'trainPredictPlot', '[', ':', ',', ':', ']', '=', 'numpy.nan', '[NEWLINE]', 'trainPredictPlot', '[', 'look_back', ':', 'len', '(', 'trainPredict', ')', '+', 'look_back', ',', ':', ']', '=', 'trainPredict', '[NEWLINE]', '[NEWLINE]', 'testPredictPlot', '=', 'numpy.empty_like', '(', 'train', ')', '[NEWLINE]', 'testPredictPlot', '[', ':', ',', ':', ']', '=', 'numpy.nan', '[NEWLINE]', 'testPredictPlot', '[', 'len', '(', 'trainPredict', ')', '+', '(', 'look_back', '*', '2', ')', '+', '1', ':', 'len', '(', 'train', ')', '-', '1', ',', ':', ']', '=', 'testPredict', '[NEWLINE]', '[NEWLINE]', 'plt.plot', '(', 'train', ')', '[NEWLINE]', 'plt.show', '(', ')', '[NEWLINE]', 'plt.plot', '(', 'trainPredictPlot', ')', '[NEWLINE]', 'plt.plot', '(', 'testPredictPlot', ')', '[NEWLINE]', 'plt.show', '(', ')', '[NEWLINE]', 'plt.plot', '(', 'train', ')', '[NEWLINE]', 'plt.plot', '(', 'trainPredictPlot', ')', '[NEWLINE]', 'plt.plot', '(', 'testPredictPlot', ')', '[NEWLINE]', 'plt.show', '(', ')', '[NEWLINE]', '[NEWLINE]', '', '']\n",
      "[]\n",
      "[]\n",
      "['%', 'matplotlib', 'inline', '[NEWLINE]', '[INDENT]', 'import', 'numpy', 'as', 'np', '[NEWLINE]', 'import', 'pandas', 'as', 'pd', '[NEWLINE]', 'from', 'fbprophet', 'import', 'Prophet', '[NEWLINE]', 'from', 'sklearn.metrics', 'import', 'mean_squared_log_error', '[NEWLINE]', '', '']\n",
      "['STORE', '=', '1', '[NEWLINE]', '[INDENT]', 'ITEM', '=', '105575', '[NEWLINE]', 'print', '(', \"'Reading data for store {}, item {}...'.format\", '(', 'STORE', ',', 'ITEM', ')', ')', '[NEWLINE]', 'it', '=', 'pd.read_csv', '(', \"'../input/train.csv'\", ',', 'iterator', '=', 'True', ',', 'chunksize', '=', '10000', ')', '[NEWLINE]', 'df', '=', 'pd.concat', '(', '[', 'c', '[', '(', 'c', '[', \"'store_nbr'\", ']', '==', 'STORE', ')', '&', '(', 'c', '[', \"'item_nbr'\", ']', '==', 'ITEM', ')', ']', 'for', 'c', 'in', 'it', ']', ')', '[NEWLINE]', 'print', '(', \"'Time-series data shape: {}'.format\", '(', 'df.shape', ')', ')', '[NEWLINE]', '', '']\n",
      "['TRAIN_SIZE', '=', '365', '[NEWLINE]', '[INDENT]', 'CV_SIZE', '=', '16', '#if you make it bigger, fill missing dates in cv with 0 if any', '[NEWLINE]', 'X', '=', 'df', '[', '-', '(', 'TRAIN_SIZE', '+', 'CV_SIZE', ')', ':', '-', 'CV_SIZE', ']', '[NEWLINE]', 'y', '=', 'df', '[', '-', 'CV_SIZE', ':', ']', '[NEWLINE]', 'print', '(', \"'Train on: {}, CV: {}'.format\", '(', 'X.shape', ',', 'y.shape', ')', ')', '[NEWLINE]', '[NEWLINE]', 'X', '=', 'X', '[', '[', \"'date'\", ',', \"'unit_sales'\", ']', ']', '[NEWLINE]', 'X.columns', '=', '[', \"'ds'\", ',', \"'y'\", ']', '#Prophet names', '[NEWLINE]', 'print', '(', 'X.tail', '(', ')', ')', '[NEWLINE]', '', '']\n",
      "['m', '=', 'Prophet', '(', 'yearly_seasonality', '=', 'True', ')', '[NEWLINE]', '[INDENT]', 'm.fit', '(', 'X', ')', '[NEWLINE]', 'print', '(', \"'Prophet fitted'\", ')', '[NEWLINE]', '', '']\n",
      "['future', '=', 'm.make_future_dataframe', '(', 'periods', '=', 'CV_SIZE', ')', '[NEWLINE]', '[INDENT]', 'pred', '=', 'm.predict', '(', 'future', ')', '[NEWLINE]', 'print', '(', 'pred', '[', '[', \"'ds'\", ',', \"'yhat'\", ',', \"'yhat_lower'\", ',', \"'yhat_upper'\", ']', '].tail', '(', '5', ')', ')', '[NEWLINE]', 'm.plot', '(', 'pred', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['m', '.plot_components', '(', 'pred', ')', '[NEWLINE]', '']\n",
      "[]\n",
      "['pred', '[', \"'ds'\", ']', '=', 'pred', '[', \"'ds'\", '].astype', '(', 'str', ')', '[NEWLINE]', '[INDENT]', 'data', '=', 'pred', '[', '[', \"'ds'\", ',', \"'yhat'\", ']', '].merge', '(', 'y', ',', 'left_on', '=', \"'ds'\", ',', 'right_on', '=', \"'date'\", ')', '[NEWLINE]', '[NEWLINE]', 'items', '=', 'pd.read_csv', '(', \"'../input/items.csv'\", ')', '#we need items for weights', '[NEWLINE]', 'items', '[', \"'weight'\", ']', '=', '1', '+', 'items', '[', \"'perishable'\", ']', '*', '0.25', '[NEWLINE]', 'data', '=', 'data.merge', '(', 'items', '[', '[', \"'item_nbr'\", ',', \"'weight'\", ']', ']', ',', 'how', '=', \"'left'\", ',', 'on', '=', \"'item_nbr'\", ')', '[NEWLINE]', '[NEWLINE]', 'score', '=', 'np.sqrt', '(', 'mean_squared_log_error', '(', 'data', '[', \"'unit_sales'\", '].clip', '(', '0', ',', '999999', ')', ',', 'data', '[', \"'yhat'\", '].fillna', '(', '0', ').clip', '(', '0', ',', '999999', ')', ',', 'data', '[', \"'weight'\", ']', ')', ')', '[NEWLINE]', 'print', '(', \"'Score:{}'.format\", '(', 'score', ')', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "['import', 'time', '[NEWLINE]', '[INDENT]', 'start_time_all', '=', 'time.time', '(', ')', '[NEWLINE]', '[NEWLINE]', 'import', 'numpy', 'as', 'np', '# linear algebra', '[NEWLINE]', 'import', 'pandas', 'as', 'pd', '# data processing, CSV file I/O (e.g. pd.read_csv)', '[NEWLINE]', '#import matplotlib.pyplot as plt', '[NEWLINE]', '[NEWLINE]', '', '']\n",
      "['# This Python 3 environment comes with many helpful analytics libraries installed', '[NEWLINE]', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', '[NEWLINE]', \"# For example, here's several helpful packages to load in \", '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', '# Input data files are available in the \"../input/\" directory.[NEWLINE]', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '[NEWLINE]', '[NEWLINE]', '[INDENT]', 'from', 'subprocess', 'import', 'check_output', '[NEWLINE]', 'print', '(', 'check_output', '(', '[', '\"ls\"', ',', '\"../input\"', ']', ').decode', '(', '\"utf8\"', ')', ')', '[NEWLINE]', 'train_set', '=', 'pd.read_csv', '(', '\"../input/train.csv\"', ')', '[NEWLINE]', '#oil_set=pd.read_csv(\"../input/oil.csv\")', '[NEWLINE]', '#mini_train=train_set', '[NEWLINE]', '#mini_train.to_csv(\"mini_train.csv\")', '[NEWLINE]', '#mini_train=pd.read_csv(\"../working/mini_train.csv\")', '[NEWLINE]', '# Any results you write to the current directory are saved as output.[NEWLINE]', 'print', '(', \"'done!'\", ')', '[NEWLINE]', '', '']\n",
      "['train_set', '.head', '(', ')', '[NEWLINE]', '']\n",
      "['size', '=', 'train_set.shape', '[', '0', ']', '[NEWLINE]', '']\n",
      "['from', 'datetime', 'import', 'datetime', '[NEWLINE]', '']\n",
      "['stores_set', '=', 'pd.read_csv', '(', '\"../input/stores.csv\"', ')', '[NEWLINE]', '#STORES ENCODE', '[NEWLINE]', '#city state type cluster', '[NEWLINE]', '[INDENT]', 'from', 'sklearn.preprocessing', 'import', 'LabelEncoder', '[NEWLINE]', 'from', 'collections', 'import', 'defaultdict', '[NEWLINE]', 'dstores', '=', 'defaultdict', '(', 'LabelEncoder', ')', '[NEWLINE]', 'df', '=', 'stores_set.copy', '(', ')', '[NEWLINE]', 'sto_nbr', '=', 'pd.DataFrame', '(', 'df', '[', \"'store_nbr'\", ']', ')', '[NEWLINE]', 'df', '=', 'df.drop', '(', \"'store_nbr'\", ',', 'axis', '=', '1', ')', '[NEWLINE]', '# Encoding the variable', '[NEWLINE]', 'fite', '=', 'df.apply', '(', 'lambda', 'x', ':', 'dstores', '[', 'x.name', '].fit_transform', '(', 'x', ')', ')', '[NEWLINE]', 'stores_encoded', '=', 'pd.merge', '(', 'sto_nbr', ',', 'fite', ',', 'left_index', '=', 'True', ',', 'right_index', '=', 'True', ')', '[NEWLINE]', 'print', '(', 'stores_encoded.head', '(', ')', ')', '[NEWLINE]', 'del', 'stores_set', '[NEWLINE]', 'del', 'fite', '[NEWLINE]', 'del', 'sto_nbr', '[NEWLINE]', 'del', 'df', '[NEWLINE]', '', '']\n",
      "['#ITEMS ENCODE', '[NEWLINE]', '#family class perishable', '[NEWLINE]', '[INDENT]', 'items_set', '=', 'pd.read_csv', '(', '\"../input/items.csv\"', ')', '[NEWLINE]', '[NEWLINE]', 'items_set.head', '(', ')', '[NEWLINE]', 'ditems', '=', 'defaultdict', '(', 'LabelEncoder', ')', '[NEWLINE]', 'dfi', '=', 'items_set.copy', '(', ')', '[NEWLINE]', 'ite_nbr', '=', 'pd.DataFrame', '(', 'dfi', '[', \"'item_nbr'\", ']', ')', '[NEWLINE]', 'dfi', '=', 'dfi.drop', '(', \"'item_nbr'\", ',', 'axis', '=', '1', ')', '[NEWLINE]', '# Encoding the variable', '[NEWLINE]', 'fite', '=', 'dfi.apply', '(', 'lambda', 'x', ':', 'ditems', '[', 'x.name', '].fit_transform', '(', 'x', ')', ')', '[NEWLINE]', 'items_encoded', '=', 'pd.merge', '(', 'ite_nbr', ',', 'fite', ',', 'left_index', '=', 'True', ',', 'right_index', '=', 'True', ')', '[NEWLINE]', 'print', '(', 'items_encoded.head', '(', ')', ')', '[NEWLINE]', 'del', 'items_set', '[NEWLINE]', 'del', 'fite', '[NEWLINE]', 'del', 'ite_nbr', '[NEWLINE]', 'del', 'dfi', '[NEWLINE]', '', '']\n",
      "['from', 'sklearn.metrics', 'import', 'mean_squared_error', '[NEWLINE]', '[INDENT]', 'from', 'math', 'import', 'sqrt', '[NEWLINE]', 'from', 'xgboost.sklearn', 'import', 'XGBRegressor', '[NEWLINE]', 'max_depth', '=', '3', '[NEWLINE]', 'min_child_weight', '=', '1', '#10', '[NEWLINE]', 'subsample', '=', '1', '#0.5', '[NEWLINE]', 'colsample_bytree', '=', '1', '#0.6', '[NEWLINE]', 'objective', '=', \"'reg:linear'\", '[NEWLINE]', 'num_estimators', '=', '1000', '[NEWLINE]', 'learning_rate', '=', '0.2', '[NEWLINE]', 'silent', '=', 'False', '[NEWLINE]', \"#booster='gblinear'\", '[NEWLINE]', 'booster', '=', \"'gbtree'\", '[NEWLINE]', 'model', '=', 'XGBRegressor', '(', '[NEWLINE]', 'max_depth', '=', 'max_depth', ',', '[NEWLINE]', 'learning_rate', '=', 'learning_rate', ',', '[NEWLINE]', 'n_estimators', '=', 'num_estimators', ',', '[NEWLINE]', 'silent', '=', 'silent', ',', '[NEWLINE]', 'objective', '=', 'objective', ',', '[NEWLINE]', 'booster', '=', 'booster', ',', '[NEWLINE]', 'n_jobs', '=', '1', ',', '[NEWLINE]', 'nthread', '=', 'None', ',', '[NEWLINE]', 'gamma', '=', '0', ',', '[NEWLINE]', 'min_child_weight', '=', 'min_child_weight', ',', '[NEWLINE]', 'max_delta_step', '=', '0', ',', '[NEWLINE]', 'subsample', '=', 'subsample', ',', '[NEWLINE]', 'colsample_bytree', '=', 'colsample_bytree', ',', '[NEWLINE]', 'colsample_bylevel', '=', '1', ',', '[NEWLINE]', 'reg_alpha', '=', '0', ',', '[NEWLINE]', 'reg_lambda', '=', '1', ',', '[NEWLINE]', 'scale_pos_weight', '=', '1', ',', '[NEWLINE]', 'base_score', '=', '0.5', ',', '[NEWLINE]', 'random_state', '=', '0', ',', 'seed', '=', 'None', ',', 'missing', '=', 'None', '[NEWLINE]', ')', '[NEWLINE]', '', '']\n",
      "['start_time', '=', 'time.time', '(', ')', '[NEWLINE]', '[INDENT]', 'size', '=', '125497040', '[NEWLINE]', 'inicio', '=', '0', '[NEWLINE]', 'interval_size', '=', '1254970', '[NEWLINE]', 'i_trainset', '=', 'interval_size', '#0#125497040#    testset 3370464', '[NEWLINE]', 'i', '=', '0', '[NEWLINE]', 'while', 'i', '<', '4', ':', '[NEWLINE]', '[INDENT]', 'i', '=', 'i', '+', '1', '[NEWLINE]', 'print', '(', 'i', ')', '[NEWLINE]', 'size', '=', 'size', '-', 'interval_size', '[NEWLINE]', 'if', 'size', '>', '0', ':', '[NEWLINE]', '[INDENT]', 'mini_train', '=', 'train_set.iloc', '[', 'inicio', ':', 'i_trainset', ',', ':', ']', '[NEWLINE]', 'inicio', '=', 'i_trainset', '[NEWLINE]', 'i_trainset', '=', 'i_trainset', '+', 'interval_size', '[NEWLINE]', 'print', '(', '\"done1\"', ')', '[NEWLINE]', 'mini_train', '=', 'mini_train.drop', '(', \"'id'\", ',', 'axis', '=', '1', ')', '[NEWLINE]', 'mini_train', '[', \"'Ano'\", ']', '=', 'mini_train', '[', \"'date'\", '].map', '(', 'lambda', 'x', ':', '(', 'datetime.strptime', '(', 'x', ',', \"'%Y-%M-%d'\", ')', ').year', ')', '[NEWLINE]', 'mini_train', '[', \"'Mes'\", ']', '=', 'mini_train', '[', \"'date'\", '].map', '(', 'lambda', 'x', ':', '(', 'datetime.strptime', '(', 'x', ',', \"'%Y-%M-%d'\", ')', ').month', ')', '[NEWLINE]', 'mini_train', '[', \"'Dia'\", ']', '=', 'mini_train', '[', \"'date'\", '].map', '(', 'lambda', 'x', ':', '(', 'datetime.strptime', '(', 'x', ',', \"'%Y-%M-%d'\", ')', ').day', ')', '[NEWLINE]', 'mini_train', '[', \"'onpromotion'\", ']', '=', '(', 'mini_train.isnull', '(', ')', '[', \"'onpromotion'\", ']', ')', '=', '0', '[NEWLINE]', 'mini_train.loc', '[', 'mini_train', '[', \"'onpromotion'\", ']', '==', 'False', ',', \"'onpromotion'\", ']', '=', '0', '[NEWLINE]', 'mini_train.loc', '[', 'mini_train', '[', \"'onpromotion'\", ']', '==', 'True', ',', \"'onpromotion'\", ']', '=', '1', '[NEWLINE]', '[NEWLINE]', 't1', '=', 'mini_train.merge', '(', 'items_encoded', ',', 'how', '=', \"'left'\", ',', 'left_on', '=', '[', \"'item_nbr'\", ']', ',', 'right_on', '=', '[', \"'item_nbr'\", ']', ')', '[NEWLINE]', 't2', '=', 't1.merge', '(', 'stores_encoded', ',', 'how', '=', \"'left'\", ',', 'left_on', '=', '[', \"'store_nbr'\", ']', ',', 'right_on', '=', '[', \"'store_nbr'\", ']', ')', '[NEWLINE]', 'df', '=', 'pd.DataFrame', '(', 't2', ')', '[NEWLINE]', 'del', 't1', '[NEWLINE]', 'del', 't2', '[NEWLINE]', 'del', 'mini_train', '[NEWLINE]', 'print', '(', '\"Done4!\"', ')', '[NEWLINE]', 'print', '(', '\"--- %s seconds ---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time', ')', ')', '[NEWLINE]', 'print', '(', '\"--- %s seconds all---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time_all', ')', ')', '[NEWLINE]', '#create train set', '[NEWLINE]', '#size=df.shape[0]', '[NEWLINE]', 'X_train', '=', 'df.loc', '[', ':', ',', '(', 'df.columns', '!=', \"'unit_sales'\", ')', '&', '(', 'df.columns', '!=', \"'date'\", ')', ']', '[NEWLINE]', 'y_train', '=', 'df.loc', '[', ':', ',', '(', 'df.columns', '==', \"'unit_sales'\", ')', ']', '[NEWLINE]', 'del', 'df', '[NEWLINE]', 'print', '(', '\"go training\"', ')', '[NEWLINE]', 'start_time', '=', 'time.time', '(', ')', '[NEWLINE]', 'model.fit', '(', 'X_train', ',', 'y_train', ')', '[NEWLINE]', 'print', '(', '\"Done\"', ')', '[NEWLINE]', 'print', '(', '\"--- %s seconds ---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time', ')', ')', '[NEWLINE]', 'print', '(', '\"--- %s seconds all---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time_all', ')', ')', '[NEWLINE]', 'print', '(', '\"train %d %d \"', '%', '(', 'inicio', ',', 'size', ')', ')', '[NEWLINE]', '', '', '', '']\n",
      "['del', 'X_train', '[NEWLINE]', '[INDENT]', 'del', 'y_train', '[NEWLINE]', '#del y_train_pred', '[NEWLINE]', 'print', '(', '\"done\"', ')', '[NEWLINE]', '', '']\n",
      "['#preparacao do set de test', '[NEWLINE]', '[INDENT]', 'start_time', '=', 'time.time', '(', ')', '[NEWLINE]', 'test_set', '=', 'pd.read_csv', '(', '\"../input/test.csv\"', ')', '[NEWLINE]', 'print', '(', 'test_set.shape', ')', '[NEWLINE]', 'test_set', '[', \"'Ano'\", ']', '=', 'test_set', '[', \"'date'\", '].map', '(', 'lambda', 'x', ':', '(', 'datetime.strptime', '(', 'x', ',', \"'%Y-%M-%d'\", ')', ').year', ')', '[NEWLINE]', 'test_set', '[', \"'Mes'\", ']', '=', 'test_set', '[', \"'date'\", '].map', '(', 'lambda', 'x', ':', '(', 'datetime.strptime', '(', 'x', ',', \"'%Y-%M-%d'\", ')', ').month', ')', '[NEWLINE]', 'test_set', '[', \"'Dia'\", ']', '=', 'test_set', '[', \"'date'\", '].map', '(', 'lambda', 'x', ':', '(', 'datetime.strptime', '(', 'x', ',', \"'%Y-%M-%d'\", ')', ').day', ')', '[NEWLINE]', 'print', '(', '\"Done!\"', ')', '[NEWLINE]', 'test_set', '[', \"'onpromotion'\", ']', '=', '(', 'test_set.isnull', '(', ')', '[', \"'onpromotion'\", ']', ')', '=', '0', '[NEWLINE]', 'test_set.loc', '[', 'test_set', '[', \"'onpromotion'\", ']', '==', 'False', ',', \"'onpromotion'\", ']', '=', '0', '[NEWLINE]', 'test_set.loc', '[', 'test_set', '[', \"'onpromotion'\", ']', '==', 'True', ',', \"'onpromotion'\", ']', '=', '1', '[NEWLINE]', 'print', '(', '\"Done!\"', ')', '[NEWLINE]', 'te1', '=', 'test_set.merge', '(', 'items_encoded', ',', 'how', '=', \"'left'\", ',', 'left_on', '=', '[', \"'item_nbr'\", ']', ',', 'right_on', '=', '[', \"'item_nbr'\", ']', ')', '[NEWLINE]', 'te2', '=', 'te1.merge', '(', 'stores_encoded', ',', 'how', '=', \"'left'\", ',', 'left_on', '=', '[', \"'store_nbr'\", ']', ',', 'right_on', '=', '[', \"'store_nbr'\", ']', ')', '[NEWLINE]', 'test_df', '=', 'pd.DataFrame', '(', 'te2', ')', '[NEWLINE]', 'X_test', '=', 'test_df.loc', '[', ':', ',', '(', 'test_df.columns', '!=', \"'id'\", ')', '&', '(', 'test_df.columns', '!=', \"'unit_sales'\", ')', '&', '(', 'test_df.columns', '!=', \"'date'\", ')', ']', '[NEWLINE]', 'del', 'te1', '[NEWLINE]', 'del', 'te2', '[NEWLINE]', 'del', 'test_set', '[NEWLINE]', 'del', 'items_encoded', '[NEWLINE]', 'del', 'stores_encoded', '[NEWLINE]', 'print', '(', '\"Done!\"', ')', '[NEWLINE]', 'print', '(', '\"--- %s seconds ---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time', ')', ')', '[NEWLINE]', '', '']\n",
      "['start_time', '=', 'time.time', '(', ')', '[NEWLINE]', '[INDENT]', 'y_test_pred', '=', 'model.predict', '(', 'X_test', ')', '[NEWLINE]', 'y_test_df', '=', 'pd.DataFrame', '(', 'y_test_pred', ',', 'columns', '=', '{', \"'unit_sales'\", '}', ')', '[NEWLINE]', 'y_test_df', '[', 'y_test_df.unit_sales', '<', '0', ']', '=', '0', '[NEWLINE]', 'output_set', '=', 'pd.concat', '(', '(', 'test_df', ',', 'y_test_df', ')', ',', 'axis', '=', '1', ')', '[', '[', \"'id'\", ',', \"'date'\", ',', \"'store_nbr'\", ',', \"'item_nbr'\", ',', \"'onpromotion'\", ',', \"'unit_sales'\", ']', ']', '[NEWLINE]', 'output_seta', '=', 'output_set.groupby', '(', '[', \"'id'\", ']', ').agg', '(', '{', \"'unit_sales'\", ':', \"'sum'\", '}', ')', '[NEWLINE]', 'output_seta.to_csv', '(', '\"output.csv\"', ')', '[NEWLINE]', 'del', 'output_set', '[NEWLINE]', 'del', 'output_seta', '[NEWLINE]', 'print', '(', \"'done!'\", ')', '[NEWLINE]', 'print', '(', '\"--- %s seconds ---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time', ')', ')', '[NEWLINE]', '', '']\n",
      "['#### THE END', '[NEWLINE]', '[NEWLINE]', '']\n",
      "['print', '(', 'check_output', '(', '[', '\"ls\"', ',', '\"../working\"', ']', ').decode', '(', '\"utf8\"', ')', ')', '[NEWLINE]', '[INDENT]', 'print', '(', '\"--- %s seconds ---\"', '%', '(', 'time.time', '(', ')', '-', 'start_time_all', ')', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "[]\n",
      "['# This Python 3 environment comes with many helpful analytics libraries installed', '[NEWLINE]', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python', '[NEWLINE]', \"# For example, here's several helpful packages to load in \", '[NEWLINE]', '[NEWLINE]', '[INDENT]', 'import', 'numpy', 'as', 'np', '# linear algebra', '[NEWLINE]', 'import', 'pandas', 'as', 'pd', '# data processing, CSV file I/O (e.g. pd.read_csv)', '[NEWLINE]', '[NEWLINE]', '# Input data files are available in the \"../input/\" directory.[NEWLINE]', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory', '[NEWLINE]', '[NEWLINE]', 'from', 'subprocess', 'import', 'check_output', '[NEWLINE]', 'print', '(', 'check_output', '(', '[', '\"ls\"', ',', '\"../input\"', ']', ').decode', '(', '\"utf8\"', ')', ')', '[NEWLINE]', '# Any results you write to the current directory are saved as output.[NEWLINE]', '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', '#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%', '[NEWLINE]', '# Load data (use only data from 2017)', '[NEWLINE]', 'train', '=', 'pd.read_csv', '(', \"'../input/train.csv'\", ',', 'skiprows', '=', 'range', '(', '1', ',', '101688780', ')', ')', '[NEWLINE]', '[NEWLINE]', '#Remove information about returned items', '[NEWLINE]', 'ind', '=', 'train', '[', \"'unit_sales'\", ']', '>=', '0', '[NEWLINE]', 'train', '=', 'train', '[', 'ind', ']', '[NEWLINE]', 'train.reset_index', '(', 'inplace', '=', 'True', ')', '#reset the indexes to account for the removed rows', '[NEWLINE]', 'train', '[', \"'date'\", ']', '=', 'pd.to_datetime', '(', 'train', '[', \"'date'\", ']', ')', '# make the date column a valid datetime object', '[NEWLINE]', '[NEWLINE]', 'stores', '=', 'pd.read_csv', '(', \"'../input/stores.csv'\", ')', '# load stores information', '[NEWLINE]', 'items', '=', 'pd.read_csv', '(', \"'../input/items.csv'\", ')', '#load information about items for sale', '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', 'grouped', '=', 'train.groupby', '(', '[', \"'item_nbr'\", ',', \"'store_nbr'\", ']', ')', '#group data by items and stores', '[NEWLINE]', 'grouped', '=', 'grouped.groups', '# get the groups', '[NEWLINE]', 'groupedKeys', '=', 'list', '(', 'grouped.keys', '(', ')', ')', '# save the indexes that forma the groups in a list', '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', 'dates', '=', 'pd.date_range', '(', 'train', '[', \"'date'\", '].iloc', '[', '0', ']', ',', 'train', '[', \"'date'\", '].iloc', '[', '-', '1', ']', ')', '# generate all dates from january 01 of 2017', '[NEWLINE]', 'itemSales', '=', 'np.zeros', '(', '(', 'len', '(', 'dates', ')', ',', 'len', '(', 'items', ')', ',', 'len', '(', 'stores', ')', ')', ')', '# matrix to save the item sales', '[NEWLINE]', '[NEWLINE]', '#here we go throgh all the groups and organize the data in the three-dimensional matrix (Future kernels will use this)', '[NEWLINE]', 'for', 'k', 'in', 'groupedKeys', ':', '[NEWLINE]', '[INDENT]', 'indItem', '=', 'np.where', '(', 'items', '[', \"'item_nbr'\", ']', '==', 'k', '[', '0', ']', ')', '# map the item_nbr to the index in the items.csv data', '[NEWLINE]', 'indStore', '=', 'k', '[', '1', ']', '# index for the store', '[NEWLINE]', '[NEWLINE]', 'index', '=', 'grouped', '[', 'k', ']', \"#indexes that form te group 'k'\", '[NEWLINE]', 'ind', '=', 'dates.searchsorted', '(', 'np.array', '(', 'train', '[', \"'date'\", '].iloc', '[', 'index', ']', ')', ')', '#every index is associated with a date, here we map the dates to our new format in the 3D matrix', '[NEWLINE]', '[NEWLINE]', 'itemSales', '[', 'ind', ',', 'indItem', ',', 'indStore', '-', '1', ']', '=', 'train', '[', \"'unit_sales'\", '].iloc', '[', 'index', ']', '#assign the value of the items.[NEWLINE]', '', '', '']\n",
      "[]\n",
      "['meanSales', '=', 'np.expm1', '(', 'np.mean', '(', 'np.log1p', '(', 'itemSales', ')', ',', 'axis', '=', '0', ')', ')', '[NEWLINE]', '']\n",
      "[]\n",
      "['test', '=', 'pd.read_csv', '(', \"'../input/test.csv'\", ')', '#load test data', '[NEWLINE]', '[NEWLINE]', '[INDENT]', 'grouped_test', '=', 'test.groupby', '(', '[', \"'item_nbr'\", ',', \"'store_nbr'\", ']', ')', '# group the data by items and stores', '[NEWLINE]', 'grouped_test', '=', 'grouped_test.groups', '[NEWLINE]', 'testkeys', '=', 'list', '(', 'grouped_test.keys', '(', ')', ')', '[NEWLINE]', '[NEWLINE]', 'predicted', '=', 'np.zeros', '(', '(', 'len', '(', 'test', ')', ',', '1', ')', ')', '# initialize the predicted output vector', '[NEWLINE]', 'for', 'k', 'in', 'testkeys', ':', '[NEWLINE]', '# for cada group look in the meanSales the prediction. note that meanSales is 4100X54 (items,stores)', '[NEWLINE]', '[INDENT]', 'indItem', '=', 'np.where', '(', 'items', '[', \"'item_nbr'\", ']', '==', 'k', '[', '0', ']', ')', '[NEWLINE]', 'indStore', '=', 'k', '[', '1', ']', '[NEWLINE]', '[NEWLINE]', 'index', '=', 'grouped_test', '[', 'k', ']', '[NEWLINE]', 'predicted', '[', 'index', ',', '0', ']', '=', 'meanSales', '[', 'indItem', ',', 'indStore', '-', '1', ']', '[NEWLINE]', '[NEWLINE]', '[NEWLINE]', '# now save the data', '[NEWLINE]', '[NEWLINE]', '', 'submit', '=', 'pd.DataFrame', '(', 'np.random.randn', '(', 'len', '(', 'test', ')', ',', '2', ')', ',', 'columns', '=', '[', \"'id'\", ',', \"'unit_sales'\", ']', ')', '[NEWLINE]', 'submit', '[', \"'id'\", ']', '=', 'test', '[', \"'id'\", ']', '[NEWLINE]', 'submit', '[', \"'unit_sales'\", ']', '=', 'predicted', '# undo the log transform', '[NEWLINE]', '[NEWLINE]', 'submit.to_csv', '(', \"'prediction_01.csv'\", ',', 'index', '=', 'False', ')', '[NEWLINE]', '', '']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['from', 'datetime', 'import', 'date', ',', 'timedelta', '[NEWLINE]', '[NEWLINE]', '[INDENT]', 'import', 'pandas', 'as', 'pd', '[NEWLINE]', 'import', 'numpy', 'as', 'np', '[NEWLINE]', 'from', 'sklearn.metrics', 'import', 'mean_squared_error', '[NEWLINE]', 'import', 'lightgbm', 'as', 'lgb', '[NEWLINE]', '', '']\n",
      "['df_train', '=', 'pd.read_csv', '(', '[NEWLINE]', \"'../input/train.csv'\", ',', 'usecols', '=', '[', '1', ',', '2', ',', '3', ',', '4', ',', '5', ']', ',', '[NEWLINE]', 'dtype', '=', '{', \"'onpromotion'\", ':', 'bool', '}', ',', '[NEWLINE]', 'converters', '=', '{', \"'unit_sales'\", ':', 'lambda', 'u', ':', 'np.log1p', '(', '[NEWLINE]', 'float', '(', 'u', ')', ')', 'if', 'float', '(', 'u', ')', '>', '0', 'else', '0', '}', ',', '[NEWLINE]', 'parse_dates', '=', '[', '\"date\"', ']', ',', '[NEWLINE]', 'skiprows', '=', 'range', '(', '1', ',', '66458909', ')', '# 2016-01-01', '[NEWLINE]', ')', '[NEWLINE]', '[INDENT]', 'df_test', '=', 'pd.read_csv', '(', '[NEWLINE]', '\"../input/test.csv\"', ',', 'usecols', '=', '[', '0', ',', '1', ',', '2', ',', '3', ',', '4', ']', ',', '[NEWLINE]', 'dtype', '=', '{', \"'onpromotion'\", ':', 'bool', '}', ',', '[NEWLINE]', 'parse_dates', '=', '[', '\"date\"', ']', '# , date_parser=parser', '[NEWLINE]', ').set_index', '(', '[NEWLINE]', '[', \"'store_nbr'\", ',', \"'item_nbr'\", ',', \"'date'\", ']', '[NEWLINE]', ')', '[NEWLINE]', 'items', '=', 'pd.read_csv', '(', '[NEWLINE]', '\"../input/items.csv\"', ',', '[NEWLINE]', ').set_index', '(', '\"item_nbr\"', ')', '[NEWLINE]', '', '']\n",
      "['items', '.head', '(', ')', '[NEWLINE]', '']\n",
      "['# Getting the observation from the 11 weeks after 2017-5-31', '[NEWLINE]', '[INDENT]', 'df_2017', '=', 'df_train', '[', 'df_train.date.isin', '(', '[NEWLINE]', 'pd.date_range', '(', '\"2017-05-31\"', ',', 'periods', '=', '7', '*', '11', ')', ')', ']', '[NEWLINE]', 'df_2017_0', '=', 'df_train', '[', 'df_train.date.isin', '(', '[NEWLINE]', 'pd.date_range', '(', '\"2017-05-31\"', ',', 'periods', '=', '7', '*', '11', ')', ')', '].copy', '(', ')', '[NEWLINE]', 'del', 'df_train', '[NEWLINE]', 'df_2017.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['promo_2017_train', '=', 'df_2017.set_index', '(', '[NEWLINE]', '[', '\"store_nbr\"', ',', '\"item_nbr\"', ',', '\"date\"', ']', ')', '[', '[', '\"onpromotion\"', ']', '].unstack', '(', '[NEWLINE]', 'level', '=', '-', '1', ').fillna', '(', 'False', ')', '[NEWLINE]', '[INDENT]', 'promo_2017_train.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['promo_2017_train', '.columns', '=', 'promo_2017_train.columns.get_level_values', '(', '1', ')', '[NEWLINE]', '[INDENT]', 'promo_2017_train.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['promo_2017_test', '=', 'df_test', '[', '[', '\"onpromotion\"', ']', '].unstack', '(', 'level', '=', '-', '1', ').fillna', '(', 'False', ')', '[NEWLINE]', '[INDENT]', 'promo_2017_test.columns', '=', 'promo_2017_test.columns.get_level_values', '(', '1', ')', '[NEWLINE]', 'promo_2017_test.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['promo_2017_test', '=', 'promo_2017_test.reindex', '(', 'promo_2017_train.index', ').fillna', '(', 'False', ')', '[NEWLINE]', '[INDENT]', 'promo_2017_test.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['# create a big table of on-promotion history, by adding the train to the test', '[NEWLINE]', '[INDENT]', 'promo_2017', '=', 'pd.concat', '(', '[', 'promo_2017_train', ',', 'promo_2017_test', ']', ',', 'axis', '=', '1', ')', '[NEWLINE]', 'promo_2017.head', '(', ')', '[NEWLINE]', 'del', 'promo_2017_test', ',', 'promo_2017_train', '[NEWLINE]', '', '']\n",
      "['df_2017', '=', 'df_2017.set_index', '(', '[NEWLINE]', '[', '\"store_nbr\"', ',', '\"item_nbr\"', ',', '\"date\"', ']', ')', '[', '[', '\"unit_sales\"', ']', '].unstack', '(', '[NEWLINE]', 'level', '=', '-', '1', ').fillna', '(', '0', ')', '[NEWLINE]', '[INDENT]', 'df_2017.columns', '=', 'df_2017.columns.get_level_values', '(', '1', ')', '[NEWLINE]', '# get_level_values(1) remove the level 0 index - unit_sales, so that the table could be isomorphic', '[NEWLINE]', 'df_2017.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['df_2017', '.head', '(', ')', '[NEWLINE]', '']\n",
      "['df_2017', '.shape', '[NEWLINE]', '']\n",
      "['# items = items.reindex(df_2017.index.get_level_values(1))', '[NEWLINE]', '[INDENT]', 'items', '=', 'items.reindex', '(', 'df_2017.index', ')', '[NEWLINE]', 'items.head', '(', ')', '[NEWLINE]', '', '']\n",
      "['items', '.shape', '[NEWLINE]', '']\n",
      "['']\n",
      "[]\n",
      "[]\n",
      "['import', 'numpy', 'as', 'np', '[NEWLINE]', '[INDENT]', 'import', 'pandas', 'as', 'pd', '[NEWLINE]', 'from', 'sklearn.metrics', 'import', 'mean_squared_error', '[NEWLINE]', '[NEWLINE]', 'from', 'subprocess', 'import', 'check_output', '[NEWLINE]', 'print', '(', 'check_output', '(', '[', '\"ls\"', ',', '\"../input\"', ']', ').decode', '(', '\"utf8\"', ')', ')', '[NEWLINE]', '', '']\n",
      "['dir_raw', '=', '\"../input/favorita-grocery-sales-forecasting/\"', '[NEWLINE]', '[INDENT]', 'dir_prep', '=', '\"../input/preparing-data-for-lgbm-or-something-else/\"', '[NEWLINE]', 'dir_1st_result', '=', '\"../input/stage-2a-the-first-half/\"', '[NEWLINE]', 'dir_2nd_result', '=', '\"../input/stage-2b-the-second-half/\"', '[NEWLINE]', 'dirs', '=', '[', 'dir_raw', ',', 'dir_prep', ',', 'dir_1st_result', ',', 'dir_2nd_result', ']', '[NEWLINE]', '', '']\n",
      "['for', 'd', 'in', 'dirs', ':', '[NEWLINE]', '[INDENT]', 'print', '(', 'd', ',', \"'\\\\n'\", ',', 'check_output', '(', '[', '\"ls\"', ',', 'd', ']', ').decode', '(', '\"utf8\"', ')', ',', \"'\\\\n'\", ')', '[NEWLINE]', '', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "95    None\n",
       "96    None\n",
       "97    None\n",
       "98    None\n",
       "99    None\n",
       "Name: tokenized_source, Length: 100, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterLibNames(row):\n",
    "    lenR = len(row)\n",
    "    # Preprocess the tokens and combine libraries with functions\n",
    "    # E.g. \n",
    "    for i in range(len(row)): \n",
    "        # If the elements before and after the . can be merged\n",
    "        if  i > 0 and i+2 < len(row) and len(row[i]) != 0 and row[i][-1] == '.':\n",
    "            row[i:(i+2)] = [''.join(row[i:(i+2)])]\n",
    "    \n",
    "\n",
    "subdf['tokenized_source'].apply(filterLibNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-sound",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

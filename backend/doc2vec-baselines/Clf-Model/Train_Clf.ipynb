{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "drawn-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from model import Generator, LibClassifier\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "cpu_cont = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "libnamesNew = np.load(\"lib_names_apr29.npy\", allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ranking-parent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 3921896 libs missing from libdict\n",
      "0 unique missing libs\n",
      "There are  19465 libs in libdict\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "k = 0\n",
    "seen = []\n",
    "libNamesNew = np.load(\"lib_names_apr29.npy\",allow_pickle=True)\n",
    "libDict = pickle.load(open(\"lib_dict_apr29.pkl\",'rb'))    \n",
    "for i in range(len(libNamesNew)):\n",
    "    for cell in libNamesNew[i]:\n",
    "        for lib in cell:\n",
    "            if lib not in libDict: \n",
    "                if lib not in seen:\n",
    "                    seen.append(lib)\n",
    "                n+=1 \n",
    "            k+=1\n",
    "print(n, 'out of', k, \"libs missing from libdict\")\n",
    "print(len(seen), \"unique missing libs\")\n",
    "\n",
    "print(\"There are \", len(libDict.keys()), \"libs in libdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-perfume",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib_names[159]\n",
    "#print(lib_dict)\n",
    "print(lib_names[58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "horizontal-flight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################TRAIN #0 EPOCH################\n",
      "train loss is:  1510.64313239529\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.0\n",
      "best eval loss is  0.0\n",
      "################TRAIN #1 EPOCH################\n",
      "train loss is:  84.69047327036186\n",
      "eval loss is:  0.2180842172067899\n",
      "best eval loss is  0.2180842172067899\n",
      "################TRAIN #2 EPOCH################\n",
      "train loss is:  76.44721226303223\n",
      "eval loss is:  0.2268906088402638\n",
      "best eval loss is  0.2268906088402638\n",
      "################TRAIN #3 EPOCH################\n",
      "train loss is:  69.85055326232214\n",
      "eval loss is:  0.24241409183312684\n",
      "best eval loss is  0.24241409183312684\n",
      "################TRAIN #4 EPOCH################\n",
      "train loss is:  64.41668752799765\n",
      "eval loss is:  0.2534982159160651\n",
      "best eval loss is  0.2534982159160651\n",
      "################TRAIN #5 EPOCH################\n",
      "train loss is:  60.003258608030855\n",
      "eval loss is:  0.27369633506123836\n",
      "best eval loss is  0.27369633506123836\n",
      "################TRAIN #6 EPOCH################\n",
      "train loss is:  56.45447334778085\n",
      "eval loss is:  0.27859958053017275\n",
      "best eval loss is  0.27859958053017275\n",
      "################TRAIN #7 EPOCH################\n",
      "train loss is:  53.563158577670386\n",
      "eval loss is:  0.2957698445480603\n",
      "best eval loss is  0.2957698445480603\n",
      "################TRAIN #8 EPOCH################\n",
      "train loss is:  51.170220218382084\n",
      "eval loss is:  0.30493872471344774\n",
      "best eval loss is  0.30493872471344774\n",
      "################TRAIN #9 EPOCH################\n",
      "train loss is:  49.10219888311757\n",
      "eval loss is:  0.3212778963339634\n",
      "best eval loss is  0.3212778963339634\n",
      "################TRAIN #10 EPOCH################\n",
      "train loss is:  47.30588160469677\n",
      "eval loss is:  0.33401072210608385\n",
      "best eval loss is  0.33401072210608385\n",
      "################TRAIN #11 EPOCH################\n",
      "train loss is:  45.71327056438666\n",
      "eval loss is:  0.33419088094662397\n",
      "best eval loss is  0.33419088094662397\n",
      "################TRAIN #12 EPOCH################\n",
      "train loss is:  44.28365115254386\n",
      "eval loss is:  0.3542520190278689\n",
      "best eval loss is  0.3542520190278689\n",
      "################TRAIN #13 EPOCH################\n",
      "train loss is:  42.9926584752981\n",
      "eval loss is:  0.36450600436864755\n",
      "best eval loss is  0.36450600436864755\n",
      "################TRAIN #14 EPOCH################\n",
      "train loss is:  41.82419528379076\n",
      "eval loss is:  0.36616419645456166\n",
      "best eval loss is  0.36616419645456166\n",
      "################TRAIN #15 EPOCH################\n",
      "train loss is:  40.757662496224036\n",
      "eval loss is:  0.38060835832968737\n",
      "best eval loss is  0.38060835832968737\n",
      "################TRAIN #16 EPOCH################\n",
      "train loss is:  39.774852393357605\n",
      "eval loss is:  0.38234199323715307\n",
      "best eval loss is  0.38234199323715307\n",
      "################TRAIN #17 EPOCH################\n",
      "train loss is:  38.868190330161546\n",
      "eval loss is:  0.3902137501117511\n",
      "best eval loss is  0.3902137501117511\n",
      "################TRAIN #18 EPOCH################\n",
      "train loss is:  38.02088216911094\n",
      "eval loss is:  0.3966699346899986\n",
      "best eval loss is  0.3966699346899986\n",
      "################TRAIN #19 EPOCH################\n",
      "train loss is:  37.23880657004548\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.3965062937293297\n",
      "best eval loss is  0.3966699346899986\n",
      "################TRAIN #20 EPOCH################\n",
      "train loss is:  36.50862647027746\n",
      "eval loss is:  0.41257389527864946\n",
      "best eval loss is  0.41257389527864946\n",
      "################TRAIN #21 EPOCH################\n",
      "train loss is:  35.8249534344714\n",
      "eval loss is:  0.4186580069936239\n",
      "best eval loss is  0.4186580069936239\n",
      "################TRAIN #22 EPOCH################\n",
      "train loss is:  35.1798706155332\n",
      "eval loss is:  0.41884583979845047\n",
      "best eval loss is  0.41884583979845047\n",
      "################TRAIN #23 EPOCH################\n",
      "train loss is:  34.56203787434257\n",
      "eval loss is:  0.4266448472172786\n",
      "best eval loss is  0.4266448472172786\n",
      "################TRAIN #24 EPOCH################\n",
      "train loss is:  33.99608891634552\n",
      "eval loss is:  0.43571781989855646\n",
      "best eval loss is  0.43571781989855646\n",
      "################TRAIN #25 EPOCH################\n",
      "train loss is:  33.44944787719083\n",
      "eval loss is:  0.44040968242364054\n",
      "best eval loss is  0.44040968242364054\n",
      "################TRAIN #26 EPOCH################\n",
      "train loss is:  32.931206678704946\n",
      "eval loss is:  0.44762436621464213\n",
      "best eval loss is  0.44762436621464213\n",
      "################TRAIN #27 EPOCH################\n",
      "train loss is:  32.43827586709875\n",
      "eval loss is:  0.45165516497232977\n",
      "best eval loss is  0.45165516497232977\n",
      "################TRAIN #28 EPOCH################\n",
      "train loss is:  31.97265968483241\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.44597582549620896\n",
      "best eval loss is  0.45165516497232977\n",
      "################TRAIN #29 EPOCH################\n",
      "train loss is:  31.52071027298757\n",
      "eval loss is:  0.4597578182434424\n",
      "best eval loss is  0.4597578182434424\n",
      "################TRAIN #30 EPOCH################\n",
      "train loss is:  31.09432090301209\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.4577412133033459\n",
      "best eval loss is  0.4597578182434424\n",
      "################TRAIN #31 EPOCH################\n",
      "train loss is:  30.687578192453554\n",
      "eval loss is:  0.46280319117582763\n",
      "best eval loss is  0.46280319117582763\n",
      "################TRAIN #32 EPOCH################\n",
      "train loss is:  30.298128517029017\n",
      "eval loss is:  0.471006109775641\n",
      "best eval loss is  0.471006109775641\n",
      "################TRAIN #33 EPOCH################\n",
      "train loss is:  29.920228362015433\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.47084268633371745\n",
      "best eval loss is  0.471006109775641\n",
      "################TRAIN #34 EPOCH################\n",
      "train loss is:  29.563350353931877\n",
      "eval loss is:  0.4720843161145846\n",
      "best eval loss is  0.4720843161145846\n",
      "################TRAIN #35 EPOCH################\n",
      "train loss is:  29.217761033069593\n",
      "eval loss is:  0.47656855602294973\n",
      "best eval loss is  0.47656855602294973\n",
      "################TRAIN #36 EPOCH################\n",
      "train loss is:  28.885332823071696\n",
      "eval loss is:  0.48119855370277015\n",
      "best eval loss is  0.48119855370277015\n",
      "################TRAIN #37 EPOCH################\n",
      "train loss is:  28.560763596672366\n",
      "eval loss is:  0.48365775988652154\n",
      "best eval loss is  0.48365775988652154\n",
      "################TRAIN #38 EPOCH################\n",
      "train loss is:  28.254394007898778\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.4828420504545554\n",
      "best eval loss is  0.48365775988652154\n",
      "################TRAIN #39 EPOCH################\n",
      "train loss is:  27.951547351212618\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.48019554099211326\n",
      "best eval loss is  0.48365775988652154\n",
      "################TRAIN #40 EPOCH################\n",
      "train loss is:  27.664953385770353\n",
      "eval loss is:  0.4912890517176726\n",
      "best eval loss is  0.4912890517176726\n",
      "################TRAIN #41 EPOCH################\n",
      "train loss is:  27.391778618828337\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.48722837273126995\n",
      "best eval loss is  0.4912890517176726\n",
      "################TRAIN #42 EPOCH################\n",
      "train loss is:  27.116912077442958\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.4911123038866581\n",
      "best eval loss is  0.4912890517176726\n",
      "################TRAIN #43 EPOCH################\n",
      "train loss is:  26.85340797975957\n",
      "eval loss is:  0.5039764036352817\n",
      "best eval loss is  0.5039764036352817\n",
      "################TRAIN #44 EPOCH################\n",
      "train loss is:  26.60216664247899\n",
      "eval loss is:  0.5059994307083961\n",
      "best eval loss is  0.5059994307083961\n",
      "################TRAIN #45 EPOCH################\n",
      "train loss is:  26.35852160034898\n",
      "eval loss is:  0.5147738625987982\n",
      "best eval loss is  0.5147738625987982\n",
      "################TRAIN #46 EPOCH################\n",
      "train loss is:  26.12328312210675\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5041806197701356\n",
      "best eval loss is  0.5147738625987982\n",
      "################TRAIN #47 EPOCH################\n",
      "train loss is:  25.8846368883517\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5097915605474741\n",
      "best eval loss is  0.5147738625987982\n",
      "################TRAIN #48 EPOCH################\n",
      "train loss is:  25.66695775090798\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5091781491270432\n",
      "best eval loss is  0.5147738625987982\n",
      "################TRAIN #49 EPOCH################\n",
      "train loss is:  25.447790373276113\n",
      "eval loss is:  0.5150694401982503\n",
      "best eval loss is  0.5150694401982503\n",
      "################TRAIN #50 EPOCH################\n",
      "train loss is:  25.23476886286847\n",
      "eval loss is:  0.5155868022869795\n",
      "best eval loss is  0.5155868022869795\n",
      "################TRAIN #51 EPOCH################\n",
      "train loss is:  25.026994159952\n",
      "eval loss is:  0.5179347959084388\n",
      "best eval loss is  0.5179347959084388\n",
      "################TRAIN #52 EPOCH################\n",
      "train loss is:  24.83019909801581\n",
      "eval loss is:  0.5210937439631194\n",
      "best eval loss is  0.5210937439631194\n",
      "################TRAIN #53 EPOCH################\n",
      "train loss is:  24.627740258838678\n",
      "eval loss is:  0.5258292811421248\n",
      "best eval loss is  0.5258292811421248\n",
      "################TRAIN #54 EPOCH################\n",
      "train loss is:  24.437809738919725\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.525033507668055\n",
      "best eval loss is  0.5258292811421248\n",
      "################TRAIN #55 EPOCH################\n",
      "train loss is:  24.250473705483245\n",
      "eval loss is:  0.5267865103406784\n",
      "best eval loss is  0.5267865103406784\n",
      "################TRAIN #56 EPOCH################\n",
      "train loss is:  24.07183989537354\n",
      "eval loss is:  0.5367638998306714\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #57 EPOCH################\n",
      "train loss is:  23.893468176917356\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5319969689234709\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #58 EPOCH################\n",
      "train loss is:  23.72150324415086\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5285970941185951\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #59 EPOCH################\n",
      "train loss is:  23.552426551725276\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.534772926186904\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #60 EPOCH################\n",
      "train loss is:  23.395329249496807\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.534023120158758\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #61 EPOCH################\n",
      "train loss is:  23.22336802398281\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5352679784481342\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #62 EPOCH################\n",
      "train loss is:  23.070466343770352\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5350447461773188\n",
      "best eval loss is  0.5367638998306714\n",
      "################TRAIN #63 EPOCH################\n",
      "train loss is:  22.913055211016605\n",
      "eval loss is:  0.5412616888299967\n",
      "best eval loss is  0.5412616888299967\n",
      "################TRAIN #64 EPOCH################\n",
      "train loss is:  22.764798491326182\n",
      "eval loss is:  0.5422066168907361\n",
      "best eval loss is  0.5422066168907361\n",
      "################TRAIN #65 EPOCH################\n",
      "train loss is:  22.615993167901134\n",
      "eval loss is:  0.5480944031706223\n",
      "best eval loss is  0.5480944031706223\n",
      "################TRAIN #66 EPOCH################\n",
      "train loss is:  22.473839216074534\n",
      "eval loss is:  0.5511414528657228\n",
      "best eval loss is  0.5511414528657228\n",
      "################TRAIN #67 EPOCH################\n",
      "train loss is:  22.325383532611696\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5436190529129444\n",
      "best eval loss is  0.5511414528657228\n",
      "################TRAIN #68 EPOCH################\n",
      "train loss is:  22.193657890565724\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5452862939391381\n",
      "best eval loss is  0.5511414528657228\n",
      "################TRAIN #69 EPOCH################\n",
      "train loss is:  22.05420984394946\n",
      "eval loss is:  0.5522618963168218\n",
      "best eval loss is  0.5522618963168218\n",
      "################TRAIN #70 EPOCH################\n",
      "train loss is:  21.925982514314494\n",
      "eval loss is:  0.5567708769669899\n",
      "best eval loss is  0.5567708769669899\n",
      "################TRAIN #71 EPOCH################\n",
      "train loss is:  21.785479719680986\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5521872716072278\n",
      "best eval loss is  0.5567708769669899\n",
      "################TRAIN #72 EPOCH################\n",
      "train loss is:  21.659571246290234\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5540556230224096\n",
      "best eval loss is  0.5567708769669899\n",
      "################TRAIN #73 EPOCH################\n",
      "train loss is:  21.5393112742146\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5514768417447041\n",
      "best eval loss is  0.5567708769669899\n",
      "################TRAIN #74 EPOCH################\n",
      "train loss is:  21.40981690827874\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5558385428709861\n",
      "best eval loss is  0.5567708769669899\n",
      "################TRAIN #75 EPOCH################\n",
      "train loss is:  21.29551240436295\n",
      "eval loss is:  0.5628182162076999\n",
      "best eval loss is  0.5628182162076999\n",
      "################TRAIN #76 EPOCH################\n",
      "train loss is:  21.17140371148\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.560001105031906\n",
      "best eval loss is  0.5628182162076999\n",
      "################TRAIN #77 EPOCH################\n",
      "train loss is:  21.06166054749584\n",
      "eval loss is:  0.566684770775147\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #78 EPOCH################\n",
      "train loss is:  20.94127758951508\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5558847742202955\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #79 EPOCH################\n",
      "train loss is:  20.829921917036746\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5652245865800442\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #80 EPOCH################\n",
      "train loss is:  20.727154972209156\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5641963558701368\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #81 EPOCH################\n",
      "train loss is:  20.607467416076748\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5643800313274066\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #82 EPOCH################\n",
      "train loss is:  20.502085778758108\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5658299453747578\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #83 EPOCH################\n",
      "train loss is:  20.395770966904678\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5658657709757487\n",
      "best eval loss is  0.566684770775147\n",
      "################TRAIN #84 EPOCH################\n",
      "train loss is:  20.291565400018598\n",
      "eval loss is:  0.5696761140456567\n",
      "best eval loss is  0.5696761140456567\n",
      "################TRAIN #85 EPOCH################\n",
      "train loss is:  20.194142910391008\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5648338480255543\n",
      "best eval loss is  0.5696761140456567\n",
      "################TRAIN #86 EPOCH################\n",
      "train loss is:  20.090869035709943\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5682820708705829\n",
      "best eval loss is  0.5696761140456567\n",
      "################TRAIN #87 EPOCH################\n",
      "train loss is:  19.996839990768173\n",
      "eval loss is:  0.5731376062983121\n",
      "best eval loss is  0.5731376062983121\n",
      "################TRAIN #88 EPOCH################\n",
      "train loss is:  19.895133647793575\n",
      "eval loss is:  0.5743867593698012\n",
      "best eval loss is  0.5743867593698012\n",
      "################TRAIN #89 EPOCH################\n",
      "train loss is:  19.8014734978684\n",
      "eval loss is:  0.5769577000767757\n",
      "best eval loss is  0.5769577000767757\n",
      "################TRAIN #90 EPOCH################\n",
      "train loss is:  19.70480634958079\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5744420480269652\n",
      "best eval loss is  0.5769577000767757\n",
      "################TRAIN #91 EPOCH################\n",
      "train loss is:  19.617835503881206\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5696996735074581\n",
      "best eval loss is  0.5769577000767757\n",
      "################TRAIN #92 EPOCH################\n",
      "train loss is:  19.520712196520922\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5752538613784007\n",
      "best eval loss is  0.5769577000767757\n",
      "################TRAIN #93 EPOCH################\n",
      "train loss is:  19.439248558186968\n",
      "eval loss is:  0.5788840625912716\n",
      "best eval loss is  0.5788840625912716\n",
      "################TRAIN #94 EPOCH################\n",
      "train loss is:  19.34593749481545\n",
      "eval loss is:  0.5832342848945886\n",
      "best eval loss is  0.5832342848945886\n",
      "################TRAIN #95 EPOCH################\n",
      "train loss is:  19.261158810298372\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5770979959613237\n",
      "best eval loss is  0.5832342848945886\n",
      "################TRAIN #96 EPOCH################\n",
      "train loss is:  19.17276390647453\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5775970111672695\n",
      "best eval loss is  0.5832342848945886\n",
      "################TRAIN #97 EPOCH################\n",
      "train loss is:  19.08618442208442\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5756153140312587\n",
      "best eval loss is  0.5832342848945886\n",
      "################TRAIN #98 EPOCH################\n",
      "train loss is:  19.00773566883085\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5821384092936149\n",
      "best eval loss is  0.5832342848945886\n",
      "################TRAIN #99 EPOCH################\n",
      "train loss is:  18.924684934458323\n",
      "Best eval, saved to disc\n",
      "eval loss is:  0.5806314474497086\n",
      "best eval loss is  0.5832342848945886\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from model import Generator, LibClassifier\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "cpu_cont = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class cacheDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    self.embed = np.load(\"embed_tensors_clean_apr29.npy\", allow_pickle=True)\n",
    "    self.kernel_ids = np.load(\"kernel_ids_apr29.npy\", allow_pickle=True)\n",
    "    self.lib_names = np.load(\"lib_names_apr29.npy\", allow_pickle=True)\n",
    "    self.lib_dict = pickle.load(open(\"lib_dict_apr29.pkl\",'rb'))    \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.embed)\n",
    "\n",
    "  def getMultiLabel(self, lib_list):\n",
    "    label = np.zeros(19465)\n",
    "    for lib in lib_list:\n",
    "        label[self.lib_dict[lib]] = 1\n",
    "    return label.reshape((1, 19465))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    embed =  np.vstack([np.zeros((1,768)), self.embed[idx]])\n",
    "    lib_name = self.lib_names[idx]\n",
    "    lib_name = np.concatenate([self.getMultiLabel(lib) for lib in lib_name])\n",
    "    # add a dummy to keep the same sequence\n",
    "    lib_name = np.vstack([np.zeros((1,19465)), lib_name])\n",
    "    return embed, lib_name\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    ## padd\n",
    "    lengths = torch.IntTensor([ embed.shape[0] for embed, _ in batch ]).to(device)\n",
    "    lengths, perm_index = lengths.sort(0, descending=True)\n",
    "    embed = torch.nn.utils.rnn.pad_sequence([ torch.Tensor(embed).to(device) for embed, _ in batch ])\n",
    "    embed = embed[:, perm_index, :]\n",
    "    lib_name = torch.nn.utils.rnn.pad_sequence([ torch.as_tensor(lib_name, dtype=torch.float, device=device) for _, lib_name in batch ])\n",
    "    lib_name = lib_name[:, perm_index, :]\n",
    "    return embed, lib_name, lengths\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "\n",
    "def train_clf(embed, lib_name, model, optimizer, lengths):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    loss = model(embed, lib_name, lengths.cpu(), criterion)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_iters(loader, model, optimizer, step_print=50):\n",
    "  count = 0\n",
    "  total = 0\n",
    "  total_loss = 0\n",
    "  for embed, lib_name, lengths in loader:\n",
    "    loss = train_clf(embed, lib_name, model, optimizer, lengths)\n",
    "    count += 1\n",
    "    total_loss += loss\n",
    "    total += 1\n",
    "    if count % step_print == 0:\n",
    "      count = 0\n",
    "      # logger.info(\"cur loss is {}\".format(loss))\n",
    "  return total_loss / total\n",
    "\n",
    "def eval(loader, model):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total = 0\n",
    "  for embed, lib_name, lengths in loader:\n",
    "    with torch.no_grad():\n",
    "      loss = model(embed, lib_name, lengths.cpu(), criterion)\n",
    "      total_loss += loss.item()\n",
    "      total += 1\n",
    "  return total_loss / total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  cache_data = cacheDataset()\n",
    "  split_size = int(len(cache_data) * 0.9)\n",
    "  train_dataset, valid_dataset = random_split(cache_data, [split_size, len(cache_data) - split_size])\n",
    "  train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn_padd, shuffle=True)\n",
    "  valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn_padd, shuffle=False)\n",
    "  save_path = \"./gen_saved_clf\"\n",
    "\n",
    "\n",
    "  gen = Generator(768, 768).to(device)\n",
    "  gen = torch.load(\"./gen_saved/best_gen.pt\").to(device)\n",
    "  clf = LibClassifier(gen, 768, 19465).to(device)\n",
    "  # clf = torch.load(\"./clf_saved/best_clf.pt\").to(device)\n",
    "  optimizer = torch.optim.Adam(clf.parameters(), lr=2e-5) \n",
    "\n",
    "  eval_loss_list = []\n",
    "\n",
    "\n",
    "  for epoch_no in range(100):\n",
    "    print(\"################TRAIN #{} EPOCH################\".format(epoch_no))\n",
    "    train_loss = train_iters(train_loader, clf, optimizer)\n",
    "    print(\"train loss is: \", train_loss)\n",
    "    eval_loss = eval(valid_loader, clf)\n",
    "    if len(eval_loss_list) == 0 or eval_loss < max(eval_loss_list):\n",
    "      print(\"Best eval, saved to disc\")\n",
    "      torch.save(clf, save_path + \"/best_clf_doc2vec.pt\") \n",
    "      torch.save(clf.state_dict(), save_path + \"/best_clf_doc2vec_state_dict.pt\")\n",
    "\n",
    "    eval_loss_list.append(eval_loss)\n",
    "    print(\"eval loss is: \", eval_loss)\n",
    "    print(\"best eval loss is \", max(eval_loss_list))\n",
    "    torch.save(clf, save_path + \"/last_clf_doc2vec.pt\")\n",
    "    torch.save(clf.state_dict(), save_path + \"/last_clf_doc2vec_state_dict.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

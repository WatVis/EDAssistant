{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from model import Generator\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.load(\"../doc2vec/data/notebooks-doc2vec-vectors-apr24.npy\", allow_pickle=True)\n",
    "A_ids = np.load(\"../doc2vec/data/notebooks-doc2vec-vectors-filenames-apr24.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################TRAIN #0 EPOCH################\n",
      "train loss is:  17.031922531216892\n",
      "Best eval, saved to disc\n",
      "eval loss is:  16.21316785812378\n",
      "best eval loss is  16.21316785812378\n",
      "################TRAIN #1 EPOCH################\n",
      "train loss is:  16.07687036804299\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.961068783700465\n",
      "best eval loss is  15.961068783700465\n",
      "################TRAIN #2 EPOCH################\n",
      "train loss is:  15.878209657633482\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.856625073899826\n",
      "best eval loss is  15.856625073899826\n",
      "################TRAIN #3 EPOCH################\n",
      "train loss is:  15.77014657943996\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.761258063465357\n",
      "best eval loss is  15.761258063465357\n",
      "################TRAIN #4 EPOCH################\n",
      "train loss is:  15.692713471935756\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.704694749663274\n",
      "best eval loss is  15.704694749663274\n",
      "################TRAIN #5 EPOCH################\n",
      "train loss is:  15.630335016481911\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.680808978776136\n",
      "best eval loss is  15.680808978776136\n",
      "################TRAIN #6 EPOCH################\n",
      "train loss is:  15.578069082391796\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.638872558623552\n",
      "best eval loss is  15.638872558623552\n",
      "################TRAIN #7 EPOCH################\n",
      "train loss is:  15.532719787821842\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.59076147004962\n",
      "best eval loss is  15.59076147004962\n",
      "################TRAIN #8 EPOCH################\n",
      "train loss is:  15.494434109374659\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.571321327984332\n",
      "best eval loss is  15.571321327984332\n",
      "################TRAIN #9 EPOCH################\n",
      "train loss is:  15.457983046325285\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.54182658145825\n",
      "best eval loss is  15.54182658145825\n",
      "################TRAIN #10 EPOCH################\n",
      "train loss is:  15.426196625428414\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.527137820671003\n",
      "best eval loss is  15.527137820671003\n",
      "################TRAIN #11 EPOCH################\n",
      "train loss is:  15.395668317800137\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.501715162893136\n",
      "best eval loss is  15.501715162893136\n",
      "################TRAIN #12 EPOCH################\n",
      "train loss is:  15.366889124930795\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.49304329926769\n",
      "best eval loss is  15.49304329926769\n",
      "################TRAIN #13 EPOCH################\n",
      "train loss is:  15.340864562721395\n",
      "eval loss is:  15.499301920334497\n",
      "best eval loss is  15.49304329926769\n",
      "################TRAIN #14 EPOCH################\n",
      "train loss is:  15.316141633845087\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.471761024246613\n",
      "best eval loss is  15.471761024246613\n",
      "################TRAIN #15 EPOCH################\n",
      "train loss is:  15.29023991147084\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.460520760218303\n",
      "best eval loss is  15.460520760218303\n",
      "################TRAIN #16 EPOCH################\n",
      "train loss is:  15.267341339543684\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.441801401972771\n",
      "best eval loss is  15.441801401972771\n",
      "################TRAIN #17 EPOCH################\n",
      "train loss is:  15.242310668995131\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.428194455554088\n",
      "best eval loss is  15.428194455554088\n",
      "################TRAIN #18 EPOCH################\n",
      "train loss is:  15.22151863708425\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.419081137577693\n",
      "best eval loss is  15.419081137577693\n",
      "################TRAIN #19 EPOCH################\n",
      "train loss is:  15.199613983061775\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.406496229022741\n",
      "best eval loss is  15.406496229022741\n",
      "################TRAIN #20 EPOCH################\n",
      "train loss is:  15.174153491632262\n",
      "eval loss is:  15.423500109215578\n",
      "best eval loss is  15.406496229022741\n",
      "################TRAIN #21 EPOCH################\n",
      "train loss is:  15.153967638513935\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.393516396731139\n",
      "best eval loss is  15.393516396731139\n",
      "################TRAIN #22 EPOCH################\n",
      "train loss is:  15.133660624498752\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.387751178691785\n",
      "best eval loss is  15.387751178691785\n",
      "################TRAIN #23 EPOCH################\n",
      "train loss is:  15.113593790513367\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.377593905478715\n",
      "best eval loss is  15.377593905478715\n",
      "################TRAIN #24 EPOCH################\n",
      "train loss is:  15.092717846827721\n",
      "eval loss is:  15.381022490064304\n",
      "best eval loss is  15.377593905478715\n",
      "################TRAIN #25 EPOCH################\n",
      "train loss is:  15.073419492858559\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.360486275454361\n",
      "best eval loss is  15.360486275454361\n",
      "################TRAIN #26 EPOCH################\n",
      "train loss is:  15.053692860834634\n",
      "eval loss is:  15.432225465277831\n",
      "best eval loss is  15.360486275454361\n",
      "################TRAIN #27 EPOCH################\n",
      "train loss is:  15.035632504011268\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.346114649871986\n",
      "best eval loss is  15.346114649871986\n",
      "################TRAIN #28 EPOCH################\n",
      "train loss is:  15.017121630373286\n",
      "eval loss is:  15.40279337738951\n",
      "best eval loss is  15.346114649871986\n",
      "################TRAIN #29 EPOCH################\n",
      "train loss is:  14.997117635474277\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.34092605387171\n",
      "best eval loss is  15.34092605387171\n",
      "################TRAIN #30 EPOCH################\n",
      "train loss is:  14.97943653366459\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.330595690757036\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #31 EPOCH################\n",
      "train loss is:  14.963217834705738\n",
      "eval loss is:  15.362487803399564\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #32 EPOCH################\n",
      "train loss is:  14.946144734300784\n",
      "eval loss is:  15.37111001610756\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #33 EPOCH################\n",
      "train loss is:  14.928178831045308\n",
      "eval loss is:  15.344496885190408\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #34 EPOCH################\n",
      "train loss is:  14.910330823775547\n",
      "eval loss is:  15.361611993114154\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #35 EPOCH################\n",
      "train loss is:  14.893227869450156\n",
      "eval loss is:  15.337082136174043\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #36 EPOCH################\n",
      "train loss is:  14.877734348400315\n",
      "eval loss is:  15.342358895142873\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #37 EPOCH################\n",
      "train loss is:  14.856803439891161\n",
      "eval loss is:  15.351532660673062\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #38 EPOCH################\n",
      "train loss is:  14.844221961809628\n",
      "eval loss is:  15.333039370179176\n",
      "best eval loss is  15.330595690757036\n",
      "################TRAIN #39 EPOCH################\n",
      "train loss is:  14.829415704109776\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.326612393806379\n",
      "best eval loss is  15.326612393806379\n",
      "################TRAIN #40 EPOCH################\n",
      "train loss is:  14.81324995453678\n",
      "eval loss is:  15.3431808454295\n",
      "best eval loss is  15.326612393806379\n",
      "################TRAIN #41 EPOCH################\n",
      "train loss is:  14.796790739271179\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.297960862517357\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #42 EPOCH################\n",
      "train loss is:  14.782918957409574\n",
      "eval loss is:  15.30115128532052\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #43 EPOCH################\n",
      "train loss is:  14.769483693944874\n",
      "eval loss is:  15.357476808379094\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #44 EPOCH################\n",
      "train loss is:  14.755718280575168\n",
      "eval loss is:  15.375257789840301\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #45 EPOCH################\n",
      "train loss is:  14.739977568387985\n",
      "eval loss is:  15.365751796464126\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #46 EPOCH################\n",
      "train loss is:  14.721068331109944\n",
      "eval loss is:  15.336327527463435\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #47 EPOCH################\n",
      "train loss is:  14.710276790741664\n",
      "eval loss is:  15.31337592030565\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #48 EPOCH################\n",
      "train loss is:  14.699572716631106\n",
      "eval loss is:  15.332312157253424\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #49 EPOCH################\n",
      "train loss is:  14.683675886534933\n",
      "eval loss is:  15.311787596096595\n",
      "best eval loss is  15.297960862517357\n",
      "################TRAIN #50 EPOCH################\n",
      "train loss is:  14.668775113660898\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.293229123204947\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #51 EPOCH################\n",
      "train loss is:  14.655849434546571\n",
      "eval loss is:  15.328454618155956\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #52 EPOCH################\n",
      "train loss is:  14.646628118050632\n",
      "eval loss is:  15.296173077325026\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #53 EPOCH################\n",
      "train loss is:  14.635289826055072\n",
      "eval loss is:  15.30760390435656\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #54 EPOCH################\n",
      "train loss is:  14.620983218301587\n",
      "eval loss is:  15.319914804150661\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #55 EPOCH################\n",
      "train loss is:  14.606697825353537\n",
      "eval loss is:  15.310731934507688\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #56 EPOCH################\n",
      "train loss is:  14.598728013127598\n",
      "eval loss is:  15.365543398757776\n",
      "best eval loss is  15.293229123204947\n",
      "################TRAIN #57 EPOCH################\n",
      "train loss is:  14.58425805195054\n",
      "Best eval, saved to disc\n",
      "eval loss is:  15.292824435979128\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #58 EPOCH################\n",
      "train loss is:  14.575428236331513\n",
      "eval loss is:  15.3490500378112\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #59 EPOCH################\n",
      "train loss is:  14.559598564212003\n",
      "eval loss is:  15.3115716509521\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #60 EPOCH################\n",
      "train loss is:  14.546770830652607\n",
      "eval loss is:  15.33505108555158\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #61 EPOCH################\n",
      "train loss is:  14.53896745149769\n",
      "eval loss is:  15.310313015182812\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #62 EPOCH################\n",
      "train loss is:  14.522100211524252\n",
      "eval loss is:  15.301334906121095\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #63 EPOCH################\n",
      "train loss is:  14.516560850748375\n",
      "eval loss is:  15.341127439836661\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #64 EPOCH################\n",
      "train loss is:  14.505395538548925\n",
      "eval loss is:  15.336413620163997\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #65 EPOCH################\n",
      "train loss is:  14.497904698350537\n",
      "eval loss is:  15.316992375999689\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #66 EPOCH################\n",
      "train loss is:  14.484327750419503\n",
      "eval loss is:  15.308341878652573\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #67 EPOCH################\n",
      "train loss is:  14.474557933522695\n",
      "eval loss is:  15.356104346861441\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #68 EPOCH################\n",
      "train loss is:  14.460075972240363\n",
      "eval loss is:  15.361370484779279\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #69 EPOCH################\n",
      "train loss is:  14.456898500670247\n",
      "eval loss is:  15.355629735440015\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #70 EPOCH################\n",
      "train loss is:  14.441950755555238\n",
      "eval loss is:  15.390425046781699\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #71 EPOCH################\n",
      "train loss is:  14.433225193130436\n",
      "eval loss is:  15.320296418915193\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #72 EPOCH################\n",
      "train loss is:  14.42708485851537\n",
      "eval loss is:  15.314560420066119\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #73 EPOCH################\n",
      "train loss is:  14.42220596035025\n",
      "eval loss is:  15.348005048930645\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #74 EPOCH################\n",
      "train loss is:  14.404688580712275\n",
      "eval loss is:  15.353301049520573\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #75 EPOCH################\n",
      "train loss is:  14.395619510937093\n",
      "eval loss is:  15.348061143358548\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #76 EPOCH################\n",
      "train loss is:  14.390642695240121\n",
      "eval loss is:  15.322540887941917\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #77 EPOCH################\n",
      "train loss is:  14.378326693132742\n",
      "eval loss is:  15.335514033337434\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #78 EPOCH################\n",
      "train loss is:  14.366279929431517\n",
      "eval loss is:  15.40303357069691\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #79 EPOCH################\n",
      "train loss is:  14.3653156397058\n",
      "eval loss is:  15.337477996200324\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #80 EPOCH################\n",
      "train loss is:  14.34471273655767\n",
      "eval loss is:  15.324994996686776\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #81 EPOCH################\n",
      "train loss is:  14.341962825006513\n",
      "eval loss is:  15.337667567282915\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #82 EPOCH################\n",
      "train loss is:  14.340086475237092\n",
      "eval loss is:  15.359461308270692\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #83 EPOCH################\n",
      "train loss is:  14.32187514807751\n",
      "eval loss is:  15.330619414150714\n",
      "best eval loss is  15.292824435979128\n",
      "################TRAIN #84 EPOCH################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9c52cc08ebf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"################TRAIN #{} EPOCH################\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train loss is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9c52cc08ebf5>\u001b[0m in \u001b[0;36mtrain_iters\u001b[0;34m(loader, model, optimizer, step_print)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9c52cc08ebf5>\u001b[0m in \u001b[0;36mtrain_gen\u001b[0;34m(data, model, optimizer, lengths)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_cosine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/smarteda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/uw/work/watvis/smarteda-mar29/notebook-eda/notebook-prediction/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, lengths, criterion, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mtgt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9c52cc08ebf5>\u001b[0m in \u001b[0;36mcriterion_cosine\u001b[0;34m(emb1, emb2)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# scores=torch.einsum(\"ij,ij->i\",emb1,emb2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# loss = torch.mean(scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from model import Generator\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "cpu_cont = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class cacheDataset(Dataset):\n",
    "  def __init__(self):\n",
    "#     self.embed = np.load(\"../doc2vec/data/notebooks-doc2vec-vectors-apr5-goodformat.npy\", allow_pickle=True)\n",
    "    self.embed = A\n",
    "#     self.kernel_ids = np.load(\"../doc2vec/data/notebooks-doc2vec-vectors-filenames-apr5.npy\", allow_pickle=True)\n",
    "    self.kernel_ids = A_ids\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.embed)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return np.vstack([np.zeros((1,768)), self.embed[idx]])\n",
    "\n",
    "def collate_fn_padd(batch):\n",
    "    ## padd\n",
    "    lengths = torch.IntTensor([ t.shape[0] for t in batch ]).to(device)\n",
    "    lengths, perm_index = lengths.sort(0, descending=True)\n",
    "    batch = torch.nn.utils.rnn.pad_sequence([ torch.Tensor(t).to(device) for t in batch ])\n",
    "    batch = batch[:, perm_index, :]\n",
    "    return batch, lengths\n",
    "\n",
    "def criterion_inner(emb1, emb2):\n",
    "    scores=torch.einsum(\"ab,cb->ac\",emb1,emb2)\n",
    "    # print(emb1.size(0))\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    loss = loss_fct(scores, torch.arange(emb1.size(0), device=scores.device))\n",
    "    return loss\n",
    "\n",
    "loss_func = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "def criterion_cosine(emb1, emb2):\n",
    "    # scores=torch.einsum(\"ij,ij->i\",emb1,emb2)\n",
    "    # loss = torch.mean(scores)\n",
    "    loss = loss_func(emb1, emb2, torch.ones(emb1.size(0)).to(device))\n",
    "    return loss\n",
    "\n",
    "def train_gen(data, model, optimizer, lengths):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    loss = model(data, lengths.cpu(), criterion_cosine)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_iters(loader, model, optimizer, step_print=50):\n",
    "  count = 0\n",
    "  total = 0\n",
    "  total_loss = 0\n",
    "  for data, lengths in loader:\n",
    "    loss = train_gen(data, model, optimizer, lengths)\n",
    "    count += 1\n",
    "    total_loss += loss\n",
    "    total += 1\n",
    "    if count % step_print == 0:\n",
    "      count = 0\n",
    "      # logger.info(\"cur loss is {}\".format(loss))\n",
    "  return total_loss / total\n",
    "\n",
    "def eval(loader, model):\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total = 0\n",
    "  for data, lengths in loader:\n",
    "    with torch.no_grad():\n",
    "      loss = model(data, lengths.cpu(), criterion_cosine)\n",
    "      total_loss += loss.item()\n",
    "      total += 1\n",
    "  return total_loss / total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  cache_data = cacheDataset()\n",
    "  split_size = int(len(cache_data) * 0.9)\n",
    "  train_dataset, valid_dataset = random_split(cache_data, [split_size, len(cache_data) - split_size])\n",
    "  train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn_padd, shuffle=True)\n",
    "  valid_loader = DataLoader(valid_dataset, batch_size=32, collate_fn=collate_fn_padd, shuffle=False)\n",
    "  save_path = \"./gen_saved\"\n",
    "\n",
    "\n",
    "  gen = Generator(768, 768).to(device)\n",
    "  # gen = torch.load(save_path + \"/last_gen.pt\")\n",
    "  optimizer_gen = torch.optim.Adam(gen.parameters(), lr=2e-5) \n",
    "\n",
    "  eval_loss_list = []\n",
    "\n",
    "\n",
    "  for epoch_no in range(100):\n",
    "    print(\"################TRAIN #{} EPOCH################\".format(epoch_no))\n",
    "    train_loss = train_iters(train_loader, gen, optimizer_gen)\n",
    "    print(\"train loss is: \", train_loss)\n",
    "    eval_loss = eval(valid_loader, gen)\n",
    "    if len(eval_loss_list) == 0 or eval_loss < min(eval_loss_list):\n",
    "      print(\"Best eval, saved to disc\")\n",
    "      torch.save(gen, save_path + \"/best_gen.pt\")\n",
    "    eval_loss_list.append(eval_loss)\n",
    "    print(\"eval loss is: \", eval_loss)\n",
    "    print(\"best eval loss is \", min(eval_loss_list))\n",
    "    torch.save(gen, save_path + \"/last_gen.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
